{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"final_bertsimas_tree_report/","title":"Notebook implementing the stable tree algorithm in Bertsimas et al. (https://arxiv.org/abs/2305.17299)","text":"In\u00a0[1]: Copied! <pre>import sys\nimport itertools\nfrom pathlib import Path\nsrc_path = Path(\"../src/dt-distance\").resolve()\nif str(src_path) not in sys.path:\n    sys.path.insert(0, str(src_path))\n\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import resample\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom dt_distance.data_processor import DataProcessor\nfrom dt_distance.tree_parser import TreeParser\nfrom dt_distance.distance_calculator import DistanceCalculator\nfrom dt_distance.problem_params import ProblemParams\nimport matplotlib.pyplot as plt\n# set seed for reproducibility\nnp.random.seed(42)\n</pre> import sys import itertools from pathlib import Path src_path = Path(\"../src/dt-distance\").resolve() if str(src_path) not in sys.path:     sys.path.insert(0, str(src_path))  import numpy as np from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import roc_auc_score from sklearn.utils import resample from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from dt_distance.data_processor import DataProcessor from dt_distance.tree_parser import TreeParser from dt_distance.distance_calculator import DistanceCalculator from dt_distance.problem_params import ProblemParams import matplotlib.pyplot as plt # set seed for reproducibility np.random.seed(42) <p>Parameters listed in the paper:</p> In\u00a0[2]: Copied! <pre>DEPTHS = list(range(3, 13))\nMIN_SAMPLES = [3, 5, 10, 30, 50]\n</pre> DEPTHS = list(range(3, 13)) MIN_SAMPLES = [3, 5, 10, 30, 50] In\u00a0[3]: Copied! <pre>data_breast_cancer = load_breast_cancer(as_frame=True)\nX_full = data_breast_cancer[\"data\"]\ny_full = data_breast_cancer[\"target\"]\nprint(\"X_full shape: \", X_full.shape)\nX_full.head()\n</pre> data_breast_cancer = load_breast_cancer(as_frame=True) X_full = data_breast_cancer[\"data\"] y_full = data_breast_cancer[\"target\"] print(\"X_full shape: \", X_full.shape) X_full.head() <pre>X_full shape:  (569, 30)\n</pre> Out[3]: mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension 0 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 ... 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 1 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 ... 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 2 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 ... 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 3 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 ... 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 4 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 ... 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 <p>5 rows \u00d7 30 columns</p> In\u00a0[4]: Copied! <pre>print(\"y_full shape: \", y_full.shape)\ny_full.head()\n</pre> print(\"y_full shape: \", y_full.shape) y_full.head() <pre>y_full shape:  (569,)\n</pre> Out[4]: <pre>0    0\n1    0\n2    0\n3    0\n4    0\nName: target, dtype: int64</pre> In\u00a0[5]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n\nprint(\"X_train shape: {}, X_test shape: {}\".format(X_train.shape, X_test.shape))\nprint(\"y_train shape: {}, y_test shape: {}\".format(y_train.shape, y_test.shape))\n</pre> X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)  print(\"X_train shape: {}, X_test shape: {}\".format(X_train.shape, X_test.shape)) print(\"y_train shape: {}, y_test shape: {}\".format(y_train.shape, y_test.shape)) <pre>X_train shape: (455, 30), X_test shape: (114, 30)\ny_train shape: (455,), y_test shape: (114,)\n</pre> In\u00a0[6]: Copied! <pre>def random_train_split(X,y):\n    N = X.shape[0]\n    indices = np.random.permutation(N)\n    X0, y0 = X[indices[:N // 2]], y[indices[:N // 2]]\n    return X0, y0\n\nX0, y0 = random_train_split(X_train.values, y_train.values)\nX0.shape, y0.shape\n</pre> def random_train_split(X,y):     N = X.shape[0]     indices = np.random.permutation(N)     X0, y0 = X[indices[:N // 2]], y[indices[:N // 2]]     return X0, y0  X0, y0 = random_train_split(X_train.values, y_train.values) X0.shape, y0.shape Out[6]: <pre>((227, 30), (227,))</pre> In\u00a0[7]: Copied! <pre>def train_decision_tree(X, y, depth, min_samples_leaf):\n    clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf)\n    clf.fit(X, y)\n    return clf\n\ndef bootstrap_trees(X, y, depths, min_samples, B):\n    '''\n    Create B bootstrap trees by sampling with replacement from X_0\n    '''\n    trees = []\n    for _ in range(B):\n        X_sample, y_sample = resample(X, y, replace= True)\n        depth = np.random.choice(depths)\n        min_leaf = np.random.choice(min_samples)\n        tree = train_decision_tree(X_sample, y_sample, depth, min_leaf)\n        trees.append(tree)\n    return trees\n\nT0 = bootstrap_trees(X0, y0, DEPTHS, MIN_SAMPLES, 100)\nprint(\"Number of trees in T0:\", len(T0))\n</pre> def train_decision_tree(X, y, depth, min_samples_leaf):     clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf)     clf.fit(X, y)     return clf  def bootstrap_trees(X, y, depths, min_samples, B):     '''     Create B bootstrap trees by sampling with replacement from X_0     '''     trees = []     for _ in range(B):         X_sample, y_sample = resample(X, y, replace= True)         depth = np.random.choice(depths)         min_leaf = np.random.choice(min_samples)         tree = train_decision_tree(X_sample, y_sample, depth, min_leaf)         trees.append(tree)     return trees  T0 = bootstrap_trees(X0, y0, DEPTHS, MIN_SAMPLES, 100) print(\"Number of trees in T0:\", len(T0)) <pre>Number of trees in T0: 100\n</pre> <p>Train the second tree collection $\\mathcal{T}$ using the same method but on the entire training set.</p> In\u00a0[8]: Copied! <pre>T = bootstrap_trees(X_train.values, y_train.values, DEPTHS, MIN_SAMPLES, 100)\nprint(\"Number of trees in T:\", len(T))\n</pre> T = bootstrap_trees(X_train.values, y_train.values, DEPTHS, MIN_SAMPLES, 100) print(\"Number of trees in T:\", len(T)) <pre>Number of trees in T: 100\n</pre> <ul> <li>For each tree in $\\mathcal{T}$, compute <code>dt-distance</code> for all $T \\in T_{0}$ and average over all B</li> <li>Compute AUC score from Test Data to get out-of-sample predictive power</li> <li>Return $B$ average distances</li> <li>Intuition for larger set: Say we get new data in the future-&gt; how much do these new trees (entire set)$\\mathcal{T}$ deviate from the previosuly smaller set of trees $T_{0}$?</li> <li>Only structural differences (via path definitions) matter for problem params, so the path_converstion does not care about the dataset, but the bounds on features, quantification of categories, and assigned class labels as a sequence of splits</li> </ul> In\u00a0[9]: Copied! <pre>def compute_average_distances(T0, T, X_train, y_train):\n    X_train_values = X_train.values if hasattr(X_train, 'values') else X_train\n    \n    distances = []\n    for i, tree_b in enumerate(T):\n        d_b = 0.0\n        for tree_beta in T0:\n            distance_calculator = DistanceCalculator(tree_beta, tree_b, X=X_train_values, y=y_train)\n            d_b += distance_calculator.compute_tree_distance()\n        d_b /= len(T0)\n        distances.append(d_b)\n    \n    return distances\ndistances = compute_average_distances(T0, T, X_train, y_train)\nprint(\"Number of distances computed:\", len(distances))\n</pre> def compute_average_distances(T0, T, X_train, y_train):     X_train_values = X_train.values if hasattr(X_train, 'values') else X_train          distances = []     for i, tree_b in enumerate(T):         d_b = 0.0         for tree_beta in T0:             distance_calculator = DistanceCalculator(tree_beta, tree_b, X=X_train_values, y=y_train)             d_b += distance_calculator.compute_tree_distance()         d_b /= len(T0)         distances.append(d_b)          return distances distances = compute_average_distances(T0, T, X_train, y_train) print(\"Number of distances computed:\", len(distances)) <pre>Number of distances computed: 100\n</pre> <p>Calculate the AUC score for each tree in $\\mathcal{T}$ using the test set.</p> In\u00a0[10]: Copied! <pre>def evaluate_predictive_power(trees, X_holdout, y_holdout):\n    auc_scores = []\n    for tree in trees:\n        y_proba = tree.predict_proba(X_holdout)[:, 1]\n        auc = roc_auc_score(y_holdout, y_proba)\n        auc_scores.append(auc)\n    return auc_scores\n\nauc_scores = evaluate_predictive_power(T, X_test.values, y_test.values)\nprint(\"Average AUC score:\", np.mean(auc_scores))\n</pre> def evaluate_predictive_power(trees, X_holdout, y_holdout):     auc_scores = []     for tree in trees:         y_proba = tree.predict_proba(X_holdout)[:, 1]         auc = roc_auc_score(y_holdout, y_proba)         auc_scores.append(auc)     return auc_scores  auc_scores = evaluate_predictive_power(T, X_test.values, y_test.values) print(\"Average AUC score:\", np.mean(auc_scores)) <pre>Average AUC score: 0.9595316082541763\n</pre> <ul> <li>Multi-objective function to find pareto optimal tree set from $\\mathcal{T}$ based on average distance, $d_{b}$ , $\\forall b \\in \\mathcal{T}$ and the out-of-sample AUC_ROC score $a_{b}$, $\\forall b \\in \\mathcal{T}$</li> <li>Pareto Optimal Definition: $(d_{b'} \\leq d_b \\text{ and } \\alpha_{b'} &gt; \\alpha_b) \\text{ or } (d_{b'} &lt; d_b \\text{ and } \\alpha_{b'} \\geq \\alpha_b)$</li> </ul> In\u00a0[11]: Copied! <pre>def pareto_optimal_trees(distances, auc_scores):\n    pareto_trees = []\n    for i, (d_i, a_i) in enumerate(zip(distances, auc_scores)):\n        dominated = False\n        for j, (d_j, a_j) in enumerate(zip(distances, auc_scores)):\n            if i != j and ((d_j &lt;= d_i and a_j &gt; a_i) or (d_j &lt; d_i and a_j &gt;= a_i)):\n                dominated = True\n                break\n        if not dominated:\n            pareto_trees.append(i)\n    return pareto_trees\n\npareto_trees = pareto_optimal_trees(distances, auc_scores)\nprint(\"Number of Pareto optimal trees:\", len(pareto_trees))\n</pre> def pareto_optimal_trees(distances, auc_scores):     pareto_trees = []     for i, (d_i, a_i) in enumerate(zip(distances, auc_scores)):         dominated = False         for j, (d_j, a_j) in enumerate(zip(distances, auc_scores)):             if i != j and ((d_j &lt;= d_i and a_j &gt; a_i) or (d_j &lt; d_i and a_j &gt;= a_i)):                 dominated = True                 break         if not dominated:             pareto_trees.append(i)     return pareto_trees  pareto_trees = pareto_optimal_trees(distances, auc_scores) print(\"Number of Pareto optimal trees:\", len(pareto_trees)) <pre>Number of Pareto optimal trees: 7\n</pre> <ul> <li>$\\mathbb{T}^\\star = \\underset{\\mathbb{T}_b \\in \\mathcal{T}^\\star}{\\text{argmax}} \\ f(d_b, \\alpha_b)$</li> <li>Need to consider here what we value: stability or predicitve power.</li> <li>Current function is most stable model among all \u201cgood enough\u201d performers.</li> <li>Can modify to find optimal trade-off for accuracy-stability</li> <li>Indicator function where:<ul> <li>1 if $\\alpha_{b}$ is within \u03b5 of the best score</li> <li>0 otherwise</li> </ul> </li> </ul> In\u00a0[12]: Copied! <pre>def select_final_tree(distances, auc_scores, pareto_indices, epsilon=0.01):\n    best_auc = max(auc_scores)\n    candidates = [i for i in pareto_indices if auc_scores[i] &gt;= (1 - epsilon) * best_auc]\n    if not candidates:\n        candidates = pareto_indices\n    best_idx = max(candidates, key=lambda i: auc_scores[i] - distances[i])\n    return best_idx\n\nselected_tree_index = select_final_tree(distances, auc_scores, pareto_trees)\nprint(\"Selected tree index:\", selected_tree_index)\n</pre> def select_final_tree(distances, auc_scores, pareto_indices, epsilon=0.01):     best_auc = max(auc_scores)     candidates = [i for i in pareto_indices if auc_scores[i] &gt;= (1 - epsilon) * best_auc]     if not candidates:         candidates = pareto_indices     best_idx = max(candidates, key=lambda i: auc_scores[i] - distances[i])     return best_idx  selected_tree_index = select_final_tree(distances, auc_scores, pareto_trees) print(\"Selected tree index:\", selected_tree_index) <pre>Selected tree index: 47\n</pre> In\u00a0[13]: Copied! <pre>selected_tree = T[selected_tree_index]\nplt.figure(figsize=(12, 8))\nplot_tree(selected_tree, \n          feature_names=X_full.columns,\n          class_names=[\"Malignant\", \"Benign\"],\n          filled=True)\nplt.title(\"Pareto Optimal Tree\")\nplt.show()\n</pre> selected_tree = T[selected_tree_index] plt.figure(figsize=(12, 8)) plot_tree(selected_tree,            feature_names=X_full.columns,           class_names=[\"Malignant\", \"Benign\"],           filled=True) plt.title(\"Pareto Optimal Tree\") plt.show() <ul> <li>Plotting pareto frontier from a collection of trees based on average distance, $d_{b}$ , $\\forall b \\in \\mathcal{T}$ and the out-of-sample AUC_ROC score $a_{b}$, $\\forall b \\in \\mathcal{T}$</li> </ul> In\u00a0[14]: Copied! <pre>def plot_pareto_frontier(distances, auc_scores, pareto_indices):\n    distances = np.array(distances)\n    auc_scores = np.array(auc_scores)\n    pareto_indices = set(pareto_indices)\n    is_pareto = np.array([i in pareto_indices for i in range(len(distances))])\n    # Plotting\n    plt.figure(figsize=(8, 6))\n    plt.scatter(distances[~is_pareto], auc_scores[~is_pareto], c='blue', label='Dominated Trees', alpha=0.6)\n    plt.scatter(distances[is_pareto], auc_scores[is_pareto], c='red', edgecolors='black', s=80, label='Pareto Optimal Trees')\n    plt.xlabel(\"Stability (Lower is Better)\")\n    plt.ylabel(\"AUC (Higher is Better)\")\n    plt.title(\"Pareto Frontier of Decision Trees\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\nplot_pareto_frontier(distances, auc_scores, pareto_trees)\n</pre> def plot_pareto_frontier(distances, auc_scores, pareto_indices):     distances = np.array(distances)     auc_scores = np.array(auc_scores)     pareto_indices = set(pareto_indices)     is_pareto = np.array([i in pareto_indices for i in range(len(distances))])     # Plotting     plt.figure(figsize=(8, 6))     plt.scatter(distances[~is_pareto], auc_scores[~is_pareto], c='blue', label='Dominated Trees', alpha=0.6)     plt.scatter(distances[is_pareto], auc_scores[is_pareto], c='red', edgecolors='black', s=80, label='Pareto Optimal Trees')     plt.xlabel(\"Stability (Lower is Better)\")     plt.ylabel(\"AUC (Higher is Better)\")     plt.title(\"Pareto Frontier of Decision Trees\")     plt.legend()     plt.grid(True)     plt.tight_layout()     plt.show()  plot_pareto_frontier(distances, auc_scores, pareto_trees) In\u00a0[15]: Copied! <pre>def plot_tree_complexity_metrics(trees):\n\n    depths = [tree.get_depth() for tree in trees]\n    node_counts = [tree.tree_.node_count for tree in trees]\n\n    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\n    # Tree Depth Plot\n    axs[0].scatter(range(len(trees)), depths, color='blue', alpha=0.7)\n    axs[0].set_title(\"Mean and Standard Deviation of Tree Depth\")\n    axs[0].set_xlabel(\"Dataset\")\n    axs[0].set_ylabel(\"Tree Depth\")\n    axs[0].grid(True)\n\n    # Node Count Plot\n    axs[1].scatter(range(len(trees)), node_counts, color='blue', alpha=0.7)\n    axs[1].set_title(\"Mean and Standard Deviation of Node Count\")\n    axs[1].set_xlabel(\"Tree Index\")\n    axs[1].set_ylabel(\"Number of Nodes\")\n    axs[1].grid(True)\n\n    plt.suptitle(\"Tree Complexity Analysis\")\n    plt.tight_layout()\n    plt.show()\nplot_tree_complexity_metrics(T)\n</pre> def plot_tree_complexity_metrics(trees):      depths = [tree.get_depth() for tree in trees]     node_counts = [tree.tree_.node_count for tree in trees]      fig, axs = plt.subplots(1, 2, figsize=(12, 5))      # Tree Depth Plot     axs[0].scatter(range(len(trees)), depths, color='blue', alpha=0.7)     axs[0].set_title(\"Mean and Standard Deviation of Tree Depth\")     axs[0].set_xlabel(\"Dataset\")     axs[0].set_ylabel(\"Tree Depth\")     axs[0].grid(True)      # Node Count Plot     axs[1].scatter(range(len(trees)), node_counts, color='blue', alpha=0.7)     axs[1].set_title(\"Mean and Standard Deviation of Node Count\")     axs[1].set_xlabel(\"Tree Index\")     axs[1].set_ylabel(\"Number of Nodes\")     axs[1].grid(True)      plt.suptitle(\"Tree Complexity Analysis\")     plt.tight_layout()     plt.show() plot_tree_complexity_metrics(T)"},{"location":"final_bertsimas_tree_report/#notebook-implementing-the-stable-tree-algorithm-in-bertsimas-et-al-httpsarxivorgabs230517299","title":"Notebook implementing the stable tree algorithm in Bertsimas et al. (https://arxiv.org/abs/2305.17299)\u00b6","text":""},{"location":"final_bertsimas_tree_report/#loading-the-breast-cancer-dataset","title":"Loading the breast cancer dataset\u00b6","text":""},{"location":"final_bertsimas_tree_report/#step-1-split-training-set-into-x0-and-x1","title":"Step 1: Split Training set into X0 and X1\u00b6","text":""},{"location":"final_bertsimas_tree_report/#step-3-bootstrap-and-train-t_0-tree-set","title":"Step 3: Bootstrap and Train ($T_{0}$) Tree Set\u00b6","text":""},{"location":"final_bertsimas_tree_report/#step-4-train-second-tree-collection-mathcalt","title":"Step 4: Train Second Tree Collection: $\\mathcal{T}$\u00b6","text":""},{"location":"final_bertsimas_tree_report/#step-51-compute-mean-distance-for-each-t-in-t","title":"Step 5.1: Compute Mean distance for each $T \\in T$\u00b6","text":""},{"location":"final_bertsimas_tree_report/#step-52-compute-out-of-sample-predicitive-performance","title":"Step 5.2: Compute out-of-sample Predicitive Performance\u00b6","text":""},{"location":"final_bertsimas_tree_report/#step-6-find-the-pareto-optimal-set-mathcalt-from-mathcalt","title":"Step 6: Find the Pareto Optimal Set $\\mathcal{T}^{*}$ from $\\mathcal{T}$\u00b6","text":""},{"location":"final_bertsimas_tree_report/#step-7-find-the-optimal-tree-from-the-pareto-optimal-set-mathcalt","title":"Step 7: Find the Optimal Tree from the Pareto Optimal Set, $\\mathcal{T^{*}}$\u00b6","text":""},{"location":"final_bertsimas_tree_report/#plot-the-pareto-optimal-tree","title":"Plot the Pareto optimal tree:\u00b6","text":""},{"location":"final_bertsimas_tree_report/#pareto-frontier-visualization","title":"Pareto Frontier Visualization\u00b6","text":""},{"location":"stable_suicide_data/","title":"bertsimas_method_suicide","text":"In\u00a0[11]: Copied! <pre>import sys\nimport itertools\nfrom pathlib import Path\nfrom typing import Dict, List\nsrc_path = Path(\"../src/dt-distance\").resolve()\ndata_path = Path(\"../data\").resolve()\nif str(src_path) not in sys.path:\n    sys.path.insert(0, str(src_path))\nsys.path.append(str(data_path))\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import resample\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom dt_distance.data_processor import DataProcessor\nfrom dt_distance.tree_parser import TreeParser\nfrom dt_distance.distance_calculator import DistanceCalculator\nfrom dt_distance.problem_params import ProblemParams\nimport matplotlib.pyplot as plt\n\n# set seed for reproducibility\nnp.random.seed(42)\n\nDEPTHS = list(range(3, 13))\nMIN_SAMPLES = [3, 5, 10, 30, 50]\n\nNUM_BOOTSTRAPS = 50\nDATA_PATH = \"../data/DataSet_Combined_SI_SNI_Baseline_FE.csv\"\n\nFEATURE_SETS: Dict[str, List[str]] = {\n    \"suicidea\": [\n        \"age\", \"gender\", \"sexori\", \"raceall\", \"trauma_sum\", \"cesd_score\", \"harddrug_life\", \"school\", \"degree\", \"job\", \"sex\", \"concurrent\", \"exchange\", \"children\", \"weapon\", \"fight\", \"fighthurt\", \"ipv\", \"ptsd_score\", \"alcfirst\", \"potfirst\", \"staycurrent\", \"homelage\", \"time_homeless_month\", \"jail\", \"jailkid\", \"gettherapy\", \"sum_alter\", \"sum_family\", \"sum_home_friends\", \"sum_street_friends\", \"sum_unknown_alter\", \"sum_talk_once_week\", \"sum_alter3close\", \"prop_family_harddrug\", \"prop_friends_harddrug\", \"prop_friends_home_harddrug\", \"prop_friends_street_harddrug\", \"prop_alter_all_harddrug\", \"prop_enc_badbehave\", \"prop_alter_homeless\", \"prop_family_emosup\", \"prop_friends_emosup\", \"prop_friends_home_emosup\", \"prop_friends_street_emosup\", \"prop_alter_all_emosup\", \"prop_family_othersupport\", \"prop_friends_othersupport\", \"prop_friends_home_othersupport\", \"prop_friends_street_othersupport\", \"prop_alter_all_othersupport\", \"sum_alter_staff\", \"prop_object_badbehave\", \"prop_enc_goodbehave\", \"prop_alter_school_job\", \"sum_alter_borrow\"],\n    \"suicattempt\": [\n        \"age\", \"gender\", \"sexori\", \"raceall\", \"trauma_sum\", \"cesd_score\", \"harddrug_life\", \"school\", \"degree\", \"job\", \"sex\", \"concurrent\", \"exchange\", \"children\", \"weapon\", \"fight\", \"fighthurt\", \"ipv\", \"ptsd_score\", \"alcfirst\", \"potfirst\", \"staycurrent\", \"homelage\", \"time_homeless_month\", \"jail\", \"jailkid\", \"gettherapy\", \"sum_alter\", \"prop_family\", \"prop_home_friends\", \"prop_street_friends\", \"prop_unknown_alter\", \"sum_talk_once_week\", \"sum_alter3close\", \"prop_family_harddrug\", \"prop_friends_harddrug\", \"prop_friends_home_harddrug\", \"prop_friends_street_harddrug\", \"prop_alter_all_harddrug\", \"prop_enc_badbehave\", \"prop_alter_homeless\", \"prop_family_emosup\", \"prop_friends_emosup\", \"prop_friends_home_emosup\", \"prop_friends_street_emosup\", \"prop_alter_all_emosup\", \"prop_family_othersupport\", \"prop_friends_othersupport\", \"prop_friends_home_othersupport\", \"prop_friends_street_othersupport\", \"prop_alter_all_othersupport\", \"sum_alter_staff\", \"prop_object_badbehave\", \"prop_enc_goodbehave\", \"prop_alter_school_job\", \"sum_alter_borrow\"],\n}\n\nMODEL_PARAMS = {\n    \"suicidea\": dict(min_samples_leaf=10, min_samples_split=20, max_depth=4),\n    \"suicattempt\": dict(min_samples_leaf=10, min_samples_split=30, max_depth=4),\n}\n\nLABELS = [\"suicidea\", \"suicattempt\"]\n</pre> import sys import itertools from pathlib import Path from typing import Dict, List src_path = Path(\"../src/dt-distance\").resolve() data_path = Path(\"../data\").resolve() if str(src_path) not in sys.path:     sys.path.insert(0, str(src_path)) sys.path.append(str(data_path))  import numpy as np import pandas as pd from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import roc_auc_score from sklearn.utils import resample from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from dt_distance.data_processor import DataProcessor from dt_distance.tree_parser import TreeParser from dt_distance.distance_calculator import DistanceCalculator from dt_distance.problem_params import ProblemParams import matplotlib.pyplot as plt  # set seed for reproducibility np.random.seed(42)  DEPTHS = list(range(3, 13)) MIN_SAMPLES = [3, 5, 10, 30, 50]  NUM_BOOTSTRAPS = 50 DATA_PATH = \"../data/DataSet_Combined_SI_SNI_Baseline_FE.csv\"  FEATURE_SETS: Dict[str, List[str]] = {     \"suicidea\": [         \"age\", \"gender\", \"sexori\", \"raceall\", \"trauma_sum\", \"cesd_score\", \"harddrug_life\", \"school\", \"degree\", \"job\", \"sex\", \"concurrent\", \"exchange\", \"children\", \"weapon\", \"fight\", \"fighthurt\", \"ipv\", \"ptsd_score\", \"alcfirst\", \"potfirst\", \"staycurrent\", \"homelage\", \"time_homeless_month\", \"jail\", \"jailkid\", \"gettherapy\", \"sum_alter\", \"sum_family\", \"sum_home_friends\", \"sum_street_friends\", \"sum_unknown_alter\", \"sum_talk_once_week\", \"sum_alter3close\", \"prop_family_harddrug\", \"prop_friends_harddrug\", \"prop_friends_home_harddrug\", \"prop_friends_street_harddrug\", \"prop_alter_all_harddrug\", \"prop_enc_badbehave\", \"prop_alter_homeless\", \"prop_family_emosup\", \"prop_friends_emosup\", \"prop_friends_home_emosup\", \"prop_friends_street_emosup\", \"prop_alter_all_emosup\", \"prop_family_othersupport\", \"prop_friends_othersupport\", \"prop_friends_home_othersupport\", \"prop_friends_street_othersupport\", \"prop_alter_all_othersupport\", \"sum_alter_staff\", \"prop_object_badbehave\", \"prop_enc_goodbehave\", \"prop_alter_school_job\", \"sum_alter_borrow\"],     \"suicattempt\": [         \"age\", \"gender\", \"sexori\", \"raceall\", \"trauma_sum\", \"cesd_score\", \"harddrug_life\", \"school\", \"degree\", \"job\", \"sex\", \"concurrent\", \"exchange\", \"children\", \"weapon\", \"fight\", \"fighthurt\", \"ipv\", \"ptsd_score\", \"alcfirst\", \"potfirst\", \"staycurrent\", \"homelage\", \"time_homeless_month\", \"jail\", \"jailkid\", \"gettherapy\", \"sum_alter\", \"prop_family\", \"prop_home_friends\", \"prop_street_friends\", \"prop_unknown_alter\", \"sum_talk_once_week\", \"sum_alter3close\", \"prop_family_harddrug\", \"prop_friends_harddrug\", \"prop_friends_home_harddrug\", \"prop_friends_street_harddrug\", \"prop_alter_all_harddrug\", \"prop_enc_badbehave\", \"prop_alter_homeless\", \"prop_family_emosup\", \"prop_friends_emosup\", \"prop_friends_home_emosup\", \"prop_friends_street_emosup\", \"prop_alter_all_emosup\", \"prop_family_othersupport\", \"prop_friends_othersupport\", \"prop_friends_home_othersupport\", \"prop_friends_street_othersupport\", \"prop_alter_all_othersupport\", \"sum_alter_staff\", \"prop_object_badbehave\", \"prop_enc_goodbehave\", \"prop_alter_school_job\", \"sum_alter_borrow\"], }  MODEL_PARAMS = {     \"suicidea\": dict(min_samples_leaf=10, min_samples_split=20, max_depth=4),     \"suicattempt\": dict(min_samples_leaf=10, min_samples_split=30, max_depth=4), }  LABELS = [\"suicidea\", \"suicattempt\"] In\u00a0[13]: Copied! <pre>def prepare_data(df: pd.DataFrame, features: List[str], label: str):\n    df = df.replace('NaN', pd.NA)  # replace the string 'NaN' with actual NaN values\n    df_full_cleaned = df[features + [label]].dropna().copy()\n    X = df_full_cleaned[features]\n    y = df_full_cleaned[label]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n    X_full = df_full_cleaned[features]\n    y_full = df_full_cleaned[label]\n    return X_full, y_full, X_train, X_test, y_train, y_test\n\ndf = pd.read_csv(DATA_PATH)\nX_full, y_full, X_train, X_test, y_train, y_test = prepare_data(df, FEATURE_SETS[\"suicidea\"], \"suicidea\")\n\nprint(f\"Number of samples in the full dataset: {len(X_full)}\")\nprint(f\"Number of samples in the training set: {len(X_train)}\")\nprint(f\"Number of samples in the test set: {len(X_test)}\")\nprint(f\"Shape of training set: {X_train.shape}\")\n</pre> def prepare_data(df: pd.DataFrame, features: List[str], label: str):     df = df.replace('NaN', pd.NA)  # replace the string 'NaN' with actual NaN values     df_full_cleaned = df[features + [label]].dropna().copy()     X = df_full_cleaned[features]     y = df_full_cleaned[label]     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)     X_full = df_full_cleaned[features]     y_full = df_full_cleaned[label]     return X_full, y_full, X_train, X_test, y_train, y_test  df = pd.read_csv(DATA_PATH) X_full, y_full, X_train, X_test, y_train, y_test = prepare_data(df, FEATURE_SETS[\"suicidea\"], \"suicidea\")  print(f\"Number of samples in the full dataset: {len(X_full)}\") print(f\"Number of samples in the training set: {len(X_train)}\") print(f\"Number of samples in the test set: {len(X_test)}\") print(f\"Shape of training set: {X_train.shape}\") <pre>Number of samples in the full dataset: 586\nNumber of samples in the training set: 439\nNumber of samples in the test set: 147\nShape of training set: (439, 56)\n</pre> In\u00a0[14]: Copied! <pre>def random_train_split(X,y):\n    N = X.shape[0]\n    indices = np.random.permutation(N)\n    X0, y0 = X[indices[:N // 2]], y[indices[:N // 2]]\n    return X0, y0\n\nX0, y0 = random_train_split(X_train.values, y_train.values)\nX0.shape, y0.shape\n</pre> def random_train_split(X,y):     N = X.shape[0]     indices = np.random.permutation(N)     X0, y0 = X[indices[:N // 2]], y[indices[:N // 2]]     return X0, y0  X0, y0 = random_train_split(X_train.values, y_train.values) X0.shape, y0.shape Out[14]: <pre>((219, 56), (219,))</pre> In\u00a0[\u00a0]: Copied! <pre>def train_decision_tree(X, y, depth, min_samples_leaf):\n    clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf)\n    clf.fit(X, y)\n    return clf\n\ndef bootstrap_trees(X, y, depths, min_samples, B):\n    '''\n    Create B bootstrap trees by sampling with replacement from X_0\n    '''\n    trees = []\n    for _ in range(B):\n        X_sample, y_sample = resample(X, y, replace= True)\n        depth = np.random.choice(depths)\n        min_leaf = np.random.choice(min_samples)\n        tree = train_decision_tree(X_sample, y_sample, depth, min_leaf)\n        trees.append(tree)\n    return trees\n\nT0 = bootstrap_trees(X0, y0, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS)\nprint(\"Number of trees in T0:\", len(T0))\n\nT = bootstrap_trees(X_train.values, y_train.values, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS)\nprint(\"Number of trees in T:\", len(T))\n</pre> def train_decision_tree(X, y, depth, min_samples_leaf):     clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf)     clf.fit(X, y)     return clf  def bootstrap_trees(X, y, depths, min_samples, B):     '''     Create B bootstrap trees by sampling with replacement from X_0     '''     trees = []     for _ in range(B):         X_sample, y_sample = resample(X, y, replace= True)         depth = np.random.choice(depths)         min_leaf = np.random.choice(min_samples)         tree = train_decision_tree(X_sample, y_sample, depth, min_leaf)         trees.append(tree)     return trees  T0 = bootstrap_trees(X0, y0, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS) print(\"Number of trees in T0:\", len(T0))  T = bootstrap_trees(X_train.values, y_train.values, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS) print(\"Number of trees in T:\", len(T)) <pre>Number of trees in T0: 50\nNumber of trees in T: 100\n</pre> In\u00a0[17]: Copied! <pre>def compute_average_distances(T0, T, X_train, y_train):\n    X_train_values = X_train.values if hasattr(X_train, 'values') else X_train\n    \n    distances = []\n    for i, tree_b in enumerate(T):\n        d_b = 0.0\n        for tree_beta in T0:\n            distance_calculator = DistanceCalculator(tree_beta, tree_b, X=X_train_values, y=y_train)\n            d_b += distance_calculator.compute_tree_distance()\n        d_b /= len(T0)\n        distances.append(d_b)\n    \n    return distances\ndistances = compute_average_distances(T0, T, X_train, y_train)\nprint(\"Number of distances computed:\", len(distances))\n</pre> def compute_average_distances(T0, T, X_train, y_train):     X_train_values = X_train.values if hasattr(X_train, 'values') else X_train          distances = []     for i, tree_b in enumerate(T):         d_b = 0.0         for tree_beta in T0:             distance_calculator = DistanceCalculator(tree_beta, tree_b, X=X_train_values, y=y_train)             d_b += distance_calculator.compute_tree_distance()         d_b /= len(T0)         distances.append(d_b)          return distances distances = compute_average_distances(T0, T, X_train, y_train) print(\"Number of distances computed:\", len(distances)) <pre>Number of distances computed: 100\n</pre> In\u00a0[18]: Copied! <pre>def evaluate_predictive_power(trees, X_holdout, y_holdout):\n    auc_scores = []\n    for tree in trees:\n        y_proba = tree.predict_proba(X_holdout)[:, 1]\n        auc = roc_auc_score(y_holdout, y_proba)\n        auc_scores.append(auc)\n    return auc_scores\n\nauc_scores = evaluate_predictive_power(T, X_test.values, y_test.values)\nprint(\"Average AUC score:\", np.mean(auc_scores))\n</pre> def evaluate_predictive_power(trees, X_holdout, y_holdout):     auc_scores = []     for tree in trees:         y_proba = tree.predict_proba(X_holdout)[:, 1]         auc = roc_auc_score(y_holdout, y_proba)         auc_scores.append(auc)     return auc_scores  auc_scores = evaluate_predictive_power(T, X_test.values, y_test.values) print(\"Average AUC score:\", np.mean(auc_scores)) <pre>Average AUC score: 0.6152495232040687\n</pre> In\u00a0[19]: Copied! <pre>def pareto_optimal_trees(distances, auc_scores):\n    pareto_trees = []\n    for i, (d_i, a_i) in enumerate(zip(distances, auc_scores)):\n        dominated = False\n        for j, (d_j, a_j) in enumerate(zip(distances, auc_scores)):\n            if i != j and ((d_j &lt;= d_i and a_j &gt; a_i) or (d_j &lt; d_i and a_j &gt;= a_i)):\n                dominated = True\n                break\n        if not dominated:\n            pareto_trees.append(i)\n    return pareto_trees\n\npareto_trees = pareto_optimal_trees(distances, auc_scores)\nprint(\"Number of Pareto optimal trees:\", len(pareto_trees))\n</pre> def pareto_optimal_trees(distances, auc_scores):     pareto_trees = []     for i, (d_i, a_i) in enumerate(zip(distances, auc_scores)):         dominated = False         for j, (d_j, a_j) in enumerate(zip(distances, auc_scores)):             if i != j and ((d_j &lt;= d_i and a_j &gt; a_i) or (d_j &lt; d_i and a_j &gt;= a_i)):                 dominated = True                 break         if not dominated:             pareto_trees.append(i)     return pareto_trees  pareto_trees = pareto_optimal_trees(distances, auc_scores) print(\"Number of Pareto optimal trees:\", len(pareto_trees)) <pre>Number of Pareto optimal trees: 6\n</pre> In\u00a0[20]: Copied! <pre>def select_final_tree(distances, auc_scores, pareto_indices, epsilon=0.01):\n    best_auc = max(auc_scores)\n    candidates = [i for i in pareto_indices if auc_scores[i] &gt;= (1 - epsilon) * best_auc]\n    if not candidates:\n        candidates = pareto_indices\n    best_idx = max(candidates, key=lambda i: auc_scores[i] - distances[i])\n    return best_idx\n\nselected_tree_index = select_final_tree(distances, auc_scores, pareto_trees)\nprint(\"Selected tree index:\", selected_tree_index)\n</pre> def select_final_tree(distances, auc_scores, pareto_indices, epsilon=0.01):     best_auc = max(auc_scores)     candidates = [i for i in pareto_indices if auc_scores[i] &gt;= (1 - epsilon) * best_auc]     if not candidates:         candidates = pareto_indices     best_idx = max(candidates, key=lambda i: auc_scores[i] - distances[i])     return best_idx  selected_tree_index = select_final_tree(distances, auc_scores, pareto_trees) print(\"Selected tree index:\", selected_tree_index) <pre>Selected tree index: 75\n</pre> In\u00a0[21]: Copied! <pre>selected_tree = T[selected_tree_index]\nplt.figure(figsize=(12, 8))\nplot_tree(selected_tree, \n          feature_names=X_full.columns,\n          class_names=[\"Malignant\", \"Benign\"],\n          filled=True)\nplt.title(\"Pareto Optimal Tree\")\nplt.show()\n</pre> selected_tree = T[selected_tree_index] plt.figure(figsize=(12, 8)) plot_tree(selected_tree,            feature_names=X_full.columns,           class_names=[\"Malignant\", \"Benign\"],           filled=True) plt.title(\"Pareto Optimal Tree\") plt.show() In\u00a0[22]: Copied! <pre>def plot_pareto_frontier(distances, auc_scores, pareto_indices):\n    distances = np.array(distances)\n    auc_scores = np.array(auc_scores)\n    pareto_indices = set(pareto_indices)\n    is_pareto = np.array([i in pareto_indices for i in range(len(distances))])\n    # Plotting\n    plt.figure(figsize=(8, 6))\n    plt.scatter(distances[~is_pareto], auc_scores[~is_pareto], c='blue', label='Dominated Trees', alpha=0.6)\n    plt.scatter(distances[is_pareto], auc_scores[is_pareto], c='red', edgecolors='black', s=80, label='Pareto Optimal Trees')\n    plt.xlabel(\"Stability (Lower is Better)\")\n    plt.ylabel(\"AUC (Higher is Better)\")\n    plt.title(\"Pareto Frontier of Decision Trees\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\nplot_pareto_frontier(distances, auc_scores, pareto_trees)\n</pre> def plot_pareto_frontier(distances, auc_scores, pareto_indices):     distances = np.array(distances)     auc_scores = np.array(auc_scores)     pareto_indices = set(pareto_indices)     is_pareto = np.array([i in pareto_indices for i in range(len(distances))])     # Plotting     plt.figure(figsize=(8, 6))     plt.scatter(distances[~is_pareto], auc_scores[~is_pareto], c='blue', label='Dominated Trees', alpha=0.6)     plt.scatter(distances[is_pareto], auc_scores[is_pareto], c='red', edgecolors='black', s=80, label='Pareto Optimal Trees')     plt.xlabel(\"Stability (Lower is Better)\")     plt.ylabel(\"AUC (Higher is Better)\")     plt.title(\"Pareto Frontier of Decision Trees\")     plt.legend()     plt.grid(True)     plt.tight_layout()     plt.show()  plot_pareto_frontier(distances, auc_scores, pareto_trees)"},{"location":"stable_suicide_data/#import-suicide-dataset","title":"Import suicide dataset\u00b6","text":""}]}