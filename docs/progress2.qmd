---
title: "Decision Tree Stability for Suicide Experience Prediction"
subtitle: "ISE 625 Project Progress"
format:
    revealjs:
        theme: [default, template/custom.scss]
        slide-number: c/t
#   clean-revealjs:
#     theme: [default, template/code.scss]
    # self-contained: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Adhithya Bhaskar
  - name: Michelle Gelman
execute: 
  cache: true
jupyter: python3
df-print: paged
date: last-modified
code-line-numbers: false
highlight-style: a11y
bibliography: refs.bib
---

## Introduction {.smaller}

Implementing stable trees proposed by Bertsimas et al. 2023 @bertsimasImprovingStabilityDecision2023

1. **Initial Training (T0):** Train initial set of decision trees on subset
2. **Full Data Training (T):** Train a second set on full training data 
3. **Distance Computation:**  Calulate average distance between trees in **T** and the trees in **T0**
  $$
  d\bigl(\mathcal{T}_{1}, \mathcal{T}_{2}\bigr) \;=\;\min_{\{x\}}\ \sum_{p\in\mathcal{P}(\mathcal{T}_{1})}\sum_{q\in\mathcal{P}(\mathcal{T}_{2})} d(p,q)\, x_{p,q} \;+\;\sum_{p\in\mathcal{P}(\mathcal{T}_{1})} w(p)\, x_{p}
  $$
1. **Performance Metrics:** Compute AUC ROC on test set
2. **Pareto Optimization:** Select Pareto optimal trees that balance predictive performance and stability
  $$
  \mathbb{T}^{\star}=\arg\!\mathrm{max}\,f\!\left(d_{b},\alpha_{b}\right)\!
  $$

::: footer
<a style="opacity: 0.2;">A</a>
:::

```{python}
import sys
import itertools
from pathlib import Path
from typing import Dict, List
from itables import show
from IPython.display import Markdown
src_path = Path("../src/dt-distance").resolve()
data_path = Path("../data").resolve()
sys.path.append(str(data_path))
sys.path.append(str(src_path))

import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import roc_auc_score
from sklearn.utils import resample
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from dt_distance.distance_calculator import DistanceCalculator
import matplotlib.pyplot as plt

# set seed for reproducibility
np.random.seed(42)
```

## Decision Tree as Collection of Paths

We represent a decision tree $ùïã$ as a set of paths:
$$
ùí´(ùïã) = \{p_1, \dots, p_T\}
$$

Each path $p$ corresponds to one leaf region in the feature space.

## Definition of a Split

- **Numerical split** on feature $j \in \mathcal{N}$ threshold $t \in [l_j,u_j]$:
  - Test: $x_j < t$ or $x_j \ge t$  
- **Categorical split** on feature $j \in \mathcal{C}$ with categories $\mathcal{C}' \subseteq [c_j]$:
  - Test: $x_j \in \mathcal{C}'$ or $x_j \notin \mathcal{C}'$

Each split partitions the feature space into two disjoint regions.

## Definition of a Tree Path

A **path** $p$ is a sequence of splits from the root to a leaf, plus the class label $k^p$.

- $\mathbf{u}^p \in \mathbb{R}^{|\mathcal{N}|}$: upper bounds for numerical features  
- $\mathbf{l}^p \in \mathbb{R}^{|\mathcal{N}|}$: lower bounds for numerical features  
- $C^p \in \{0,1\}^{|\mathcal{C}| \times \max_j c_j}$: category masks  
- $k^p \in [K]$: assigned class label

Thus:  
$$
(\mathbf{u}^p,\mathbf{l}^p,C^p,k^p) \in \mathbb{R}^{|\mathcal{N}|} \times \mathbb{R}^{|\mathcal{N}|} \times \{0,1\}^{|\mathcal{C}| \times \max_j c_j} \times [K]
$$

## Explanations of components

- **$\mathbf{u}^p_j$**, **$\mathbf{l}^p_j$**: define the numeric interval for feature $j$  
- **$c^p_j$**: binary vector indicating which categories of feature $j$ satisfy the splits  
- **$C^p$**: matrix stacking all $c^p_j$  
- **$k^p$**: class label predicted for this region

---

## Tree Representation & Non-Uniqueness

- Order of splits is irrelevant; different trees can yield the same set of paths.
- Desirable: identical path sets imply identical decision logic regardless of tree shape.

## Path Distance {.smaller}
Compare feature ranges and class weight
$$
    d(p,q) = \sum_{j \in \mathcal{N}} \frac{\left|u^p_j - u^q_j\right| + \left|l^p_j - l^q_j\right|}{2(u_j - l_j)} + \sum_{j \in \mathcal{C}} \frac{\left\| c^p_j-c^q_j\right\|_1}{c_j} + \lambda \cdot 1_{(k^p \neq k^q)}
$$

## Path Weight {.smaller}
- Portion of feature ranges captured by the path
$$
    d(p,q) = \sum_{j \in \mathcal{N}} \frac{\left|u^p_j - u^q_j\right| + \left|l^p_j - l^q_j\right|}{2(u_j - l_j)} + \sum_{j \in \mathcal{C}} \frac{\left\| c^p_j-c^q_j\right\|_1}{c_j} + \lambda \cdot 1_{(k^p \neq k^q)}
$$

## Tree Distance {.smaller}
 $$
\begin{align*}
d({T}_1, {T}_2)
&= \min_{{x}} \Bigg[ 
    \sum_{p \in \mathcal{P}({T}_1)} \sum_{q \in \mathcal{P}(\mathbb{T}_2)} d(p,q) \, x_{pq} 
    + \sum_{p \in \mathcal{P}(\mathbb{T}_1)} w(p) \, x_{p} 
\Bigg] \\
\text{s.t.} \quad 
& \sum_{q \in \mathcal{P}(\mathbb{T}_2)} x_{pq} + x_{p} = 1, 
\quad \forall p \in \mathcal{P}(\mathbb{T}_1) \\
& \sum_{p \in \mathcal{P}(\mathbb{T}_1)} x_{pq} = 1, 
\quad \forall q \in \mathcal{P}(\mathbb{T}_2) \\
& x_{pq} \in \{0,1\}, \quad x_{p} \in \{0,1\}, 
\quad \forall p \in \mathcal{P}(\mathbb{T}_1), 
\quad \forall q \in \mathcal{P}(\mathbb{T}_2)
\end{align*}
$$

## Configuration Parameters

```{python}
#| echo: True
DEPTHS = list(range(3, 13))
MIN_SAMPLES = [3, 5, 10, 30, 50]

NUM_BOOTSTRAPS = 25
DATA_PATH = "../data/DataSet_Combined_SI_SNI_Baseline_FE.csv"

FEATURE_SETS: Dict[str, List[str]] = {
    "suicidea": [
        "age", "gender", "sexori", "raceall", "trauma_sum", "cesd_score", "harddrug_life", "school", "degree", "job", "sex", "concurrent", "exchange", "children", "weapon", "fight", "fighthurt", "ipv", "ptsd_score", "alcfirst", "potfirst", "staycurrent", "homelage", "time_homeless_month", "jail", "jailkid", "gettherapy", "sum_alter", "sum_family", "sum_home_friends", "sum_street_friends", "sum_unknown_alter", "sum_talk_once_week", "sum_alter3close", "prop_family_harddrug", "prop_friends_harddrug", "prop_friends_home_harddrug", "prop_friends_street_harddrug", "prop_alter_all_harddrug", "prop_enc_badbehave", "prop_alter_homeless", "prop_family_emosup", "prop_friends_emosup", "prop_friends_home_emosup", "prop_friends_street_emosup", "prop_alter_all_emosup", "prop_family_othersupport", "prop_friends_othersupport", "prop_friends_home_othersupport", "prop_friends_street_othersupport", "prop_alter_all_othersupport", "sum_alter_staff", "prop_object_badbehave", "prop_enc_goodbehave", "prop_alter_school_job", "sum_alter_borrow"],
    "suicattempt": [
        "age", "gender", "sexori", "raceall", "trauma_sum", "cesd_score", "harddrug_life", "school", "degree", "job", "sex", "concurrent", "exchange", "children", "weapon", "fight", "fighthurt", "ipv", "ptsd_score", "alcfirst", "potfirst", "staycurrent", "homelage", "time_homeless_month", "jail", "jailkid", "gettherapy", "sum_alter", "prop_family", "prop_home_friends", "prop_street_friends", "prop_unknown_alter", "sum_talk_once_week", "sum_alter3close", "prop_family_harddrug", "prop_friends_harddrug", "prop_friends_home_harddrug", "prop_friends_street_harddrug", "prop_alter_all_harddrug", "prop_enc_badbehave", "prop_alter_homeless", "prop_family_emosup", "prop_friends_emosup", "prop_friends_home_emosup", "prop_friends_street_emosup", "prop_alter_all_emosup", "prop_family_othersupport", "prop_friends_othersupport", "prop_friends_home_othersupport", "prop_friends_street_othersupport", "prop_alter_all_othersupport", "sum_alter_staff", "prop_object_badbehave", "prop_enc_goodbehave", "prop_alter_school_job", "sum_alter_borrow"],
}

MODEL_PARAMS = {
    "suicidea": dict(min_samples_leaf=10, min_samples_split=20, max_depth=4),
    "suicattempt": dict(min_samples_leaf=10, min_samples_split=30, max_depth=4),
}

LABELS = ["suicidea", "suicattempt"]
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Data Import and Preparation

```{python}
#| label: import-data
#| echo: true
def prepare_data(df: pd.DataFrame, features: List[str], label: str):
    df = df.replace('NaN', pd.NA)  # replace the string 'NaN' with actual NaN values
    df_full_cleaned = df[features + [label]].dropna().copy()
    X = df_full_cleaned[features]
    y = df_full_cleaned[label]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)
    X_full = df_full_cleaned[features]
    y_full = df_full_cleaned[label]
    return X_full, y_full, X_train, X_test, y_train, y_test

df = pd.read_csv(DATA_PATH)
X_full, y_full, X_train, X_test, y_train, y_test = prepare_data(df, FEATURE_SETS["suicidea"], "suicidea")
```
```{python}
#| echo: false
print(f"Number of samples in the full dataset: {len(X_full)}")
print(f"Number of samples in the training set: {len(X_train)}")
print(f"Number of samples in the test set: {len(X_test)}")
print(f"Shape of training set: {X_train.shape}")
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## A peek at the data {.smaller}

```{python}     
#! echo: true
show(X_train, searching = False, paging = True,ordering = False, autoWidth = True)
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Random Train Split

```{python}
#| label: train-test-split
#| echo: true
def random_train_split(X, y):
    N = X.shape[0]
    indices = np.random.permutation(N)
    X0, y0 = X[indices[:N // 2]], y[indices[:N // 2]]
    return X0, y0

X0, y0 = random_train_split(X_train.values, y_train.values)
X0.shape, y0.shape
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Decision Tree Training Functions


```{python}
#| echo: true
#| code-line-numbers: "|6,7|10|11,12"
def train_decision_tree(X, y, depth, min_samples_leaf):
    clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf)
    clf.fit(X, y)
    return clf

def bootstrap_trees(X, y, depths, min_samples, B):
    # Create B bootstrap trees by sampling with replacement from X_0
    trees = []
    for _ in range(B):
        X_sample, y_sample = resample(X, y, replace=True)
        depth = np.random.choice(depths)
        min_leaf = np.random.choice(min_samples)
        tree = train_decision_tree(X_sample, y_sample, depth, min_leaf)
        trees.append(tree)
    return trees
```

## Bootstrapping Decision Trees

```{python}
#| echo: true
T0 = bootstrap_trees(X0, y0, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS)
print("Number of trees in T0:", len(T0))

T = bootstrap_trees(X_train.values, y_train.values, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS)
print("Number of trees in T:", len(T))
```

## Computing Tree Distances

```{python}
#| eval: false
#| echo: true
#| code-line-numbers: "|5|8,9"
def compute_average_distances(T0, T, X_train, y_train):
    X_train_values = X_train.values if hasattr(X_train, 'values') else X_train
    
    distances = []
    for i, tree_b in enumerate(T):
        d_b = 0.0
        for tree_beta in T0:
            distance_calculator = DistanceCalculator(tree_beta, tree_b, X=X_train_values, y=y_train)
            d_b += distance_calculator.compute_tree_distance()
        d_b /= len(T0)
        distances.append(d_b)
    
    return distances
distances = compute_average_distances(T0, T, X_train, y_train)
```

## Evaluating Predictive Power

```{python}
#| eval: true
# load the distances from a csv
distances = []
with open("distances.csv", "r") as f:
    for line in f:
        distances.append(float(line.strip()))
```

```{python}
#| eval: false
# save the distances into a csv
with open("distances.csv", "w") as f:
    for distance in distances:
        f.write(f"{distance}\n")
```

```{python}
#| echo: true
def evaluate_predictive_power(trees, X_holdout, y_holdout):
    auc_scores = []
    for tree in trees:
        y_prob = tree.predict_proba(X_holdout)[:, 1]
        auc = roc_auc_score(y_holdout, y_prob)
        auc_scores.append(auc)
    return auc_scores

auc_scores = evaluate_predictive_power(T, X_test.values, y_test.values)
print("Average AUC score:", np.mean(auc_scores))
```

## Finding Pareto Optimal Trees

- **Pareto Optimal:** $(d_{b'} \leq d_b \text{ and } \alpha_{b'} > \alpha_b) \text{ or } (d_{b'} < d_b \text{ and } \alpha_{b'} \geq \alpha_b)$

```{python}
#| echo: true
#| code-line-numbers: "|5,6"
def pareto_optimal_trees(distances, auc_scores):
    pareto_trees = []
    for i, (d_i, a_i) in enumerate(zip(distances, auc_scores)):
        dominated = False
        for j, (d_j, a_j) in enumerate(zip(distances, auc_scores)):
            if i != j and ((d_j <= d_i and a_j > a_i) or (d_j < d_i and a_j >= a_i)):
                dominated = True
                break
        if not dominated:
            pareto_trees.append(i)
    return pareto_trees

pareto_trees = pareto_optimal_trees(distances, auc_scores)
print("Number of Pareto optimal trees:", len(pareto_trees))
```

## Tree Selection Strategy

-  $\mathbb{T}^\star = \underset{\mathbb{T}_b \in \mathcal{T}^\star}{\text{argmax}} \ f(d_b, \alpha_b)$
- Stability or predictive power?
- Indicator function: $\alpha_{b}$ is within Œµ of the best score

```{python}
#| echo: true
#| code-line-numbers: "|3"
def select_final_tree(distances, auc_scores, pareto_indices, epsilon=0.01):
    best_auc = max(auc_scores)
    candidates = [i for i in pareto_indices if auc_scores[i] >= (1 - epsilon) * best_auc]
    if not candidates:
        candidates = pareto_indices
    best_idx = max(candidates, key=lambda i: auc_scores[i] - distances[i])
    return best_idx

selected_tree_index = select_final_tree(distances, auc_scores, pareto_trees)
print("Selected tree index:", selected_tree_index)
```

## Visualizing Selected Tree

```{python}
#| align: center
selected_tree = T[selected_tree_index]
plt.figure(figsize=(12, 8), dpi=300)
plot_tree(selected_tree, 
          feature_names=X_full.columns,
          class_names=["No Suicide Ideation", "Suicide Ideation"],
          filled=True)
plt.tight_layout()
```

## Pareto Frontier Visualization

:::: {.columns}

::: {.column width="50%"}
```{python}
def plot_pareto_frontier(distances, auc_scores, pareto_indices):
    distances = np.array(distances)
    auc_scores = np.array(auc_scores)
    pareto_indices = set(pareto_indices)
    is_pareto = np.array([i in pareto_indices for i in range(len(distances))])
    # Plotting
    plt.figure(figsize=(8, 6), dpi=300)
    plt.scatter(distances[~is_pareto], auc_scores[~is_pareto], c='blue', label='Dominated Trees', alpha=0.6)
    plt.scatter(distances[is_pareto], auc_scores[is_pareto], c='red', edgecolors='black', s=80, label='Pareto Optimal Trees')
    plt.xlabel("Stability (Lower is Better)")
    plt.ylabel("AUC (Higher is Better)")
    plt.title("Pareto Frontier of Decision Trees")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

plot_pareto_frontier(distances, auc_scores, pareto_trees)
```
:::

::: {.column width="50%"}

- $d_{b}$ , $\forall b \in \mathcal{T}$: Average distance
- $a_{b}$, $\forall b \in \mathcal{T}$: Out-of-sample AUCROC
:::

::::

## Experiment runner

![](img/carbon.png){height=600, fig-align="center"}

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Thoughts and points of dicussion {.smaller}

1. Does the sequence of paths matter? @10571918

> The study of [39] proposes a new distance metric to quantify the structural differences and prediction similarities between decision trees. ... However, the metric does not consider the sequence of splits, thereby potentially overlooking the overall structural similarity of the trees. Furthermore, the approach suffers from high computational complexity due to the need for pairwise comparison of all paths between two trees, which can become computationally expensive as the number of leaf nodes increases.

2. Stability-performance trade-off when selecting Pareto Optimal tree?

## References

