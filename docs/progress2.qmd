---
title: "Decision Tree Stability for Suicide Experience Prediction"
subtitle: "ISE 625 Project Presentation"
format:
    revealjs:
        theme: [default, template/custom.scss]
        slide-number: c/t
#   clean-revealjs:
#     theme: [default, template/code.scss]
    # self-contained: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: "**Adhithya Bhaskar**"
  - name: "**Michelle Gelman**"
execute: 
  cache: false
jupyter: python3
df-print: paged
date: last-modified
code-line-numbers: false
highlight-style: a11y
bibliography: refs.bib
csl: template/ieee.csl
---

## Problem outline

- 4.2 million youth experience homelessness each year in the U.S
- **~27%** and **~35%** of **R**unaway and **H**omeless **Y**outhreport past-year suicidal ideation @kirstConcurrentMentalHealth2011 @rewSexualAbuseAlcohol2001
- **15.8%** in general population @YouthRiskBehaviora
- **>50%** of RHY have experienced suicidal ideation during their lifetime @merschamMentalHealthSubstance2009 @molnarSuicidalBehaviorSexual1998 @vottaSuicideHighriskBehaviors2004

## Key question

- Most studies focus on **individual characteristics** in suicidal risk for YEH @PreventingSuicideConnectedness
- Limited insight on relevant factors and combinations of both individual and social factors that influence suicide risk @fulginitiRiskyIntegrationSocial2016

**Can a stable tree be used to understand the clinical risk profiles of YEH?**

## Stable decision trees {.smaller}

Implementing stable trees proposed by Bertsimas et al. 2023 @bertsimasImprovingStabilityDecision2023

1. **Initial Training (T0):** Train initial set of decision trees on subset
2. **Full Data Training (T):** Train a second set on full training data 
3. **Distance Computation:**  Calulate average distance between trees in **T** and the trees in **T0**
  $$
  d\bigl(\mathcal{T}_{1}, \mathcal{T}_{2}\bigr) \;=\;\min_{\{x\}}\ \sum_{p\in\mathcal{P}(\mathcal{T}_{1})}\sum_{q\in\mathcal{P}(\mathcal{T}_{2})} d(p,q)\, x_{p,q} \;+\;\sum_{p\in\mathcal{P}(\mathcal{T}_{1})} w(p)\, x_{p}
  $$
1. **Performance Metrics:** Compute AUC ROC on test set
2. **Pareto Optimization:** Select Pareto optimal trees that balance predictive performance and stability
  $$
  \mathbb{T}^{\star}=\arg\!\mathrm{max}\,f\!\left(d_{b},\alpha_{b}\right)\!
  $$

::: footer
<a style="opacity: 0.2;">A</a>
:::

```{python}
import sys
import itertools
from pathlib import Path
from typing import Dict, List
from itables import show
from IPython.display import Markdown
src_path = Path("../src/dt-distance").resolve()
data_path = Path("../data").resolve()
sys.path.append(str(data_path))
sys.path.append(str(src_path))

import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import roc_auc_score
from sklearn.utils import resample
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from dt_distance.distance_calculator import DistanceCalculator
import matplotlib.pyplot as plt
from alive_progress import alive_bar
from imblearn.over_sampling import SMOTE, SVMSMOTE
# set seed for reproducibility
np.random.seed(42)
```

## Tree Path

*Sequence of splits from the root to a leaf & class label $k^p$*

$$
ùí´(ùïã) = \{p_1, \dots, p_T\}
$$

- **$\mathbf{u}^p_j$**, **$\mathbf{l}^p_j$**: define the numeric interval for feature $j$  
- **$\mathbf{c}^p_j$**: binary vector indicating which categories of feature $j$ satisfy the splits  
- **$\mathbf{k}^p$**: class label predicted for this region


## Tree Distance {.smaller}
 $$
\begin{align*}
d({T}_1, {T}_2)
&= \min_{{x}} \Bigg[ 
    \sum_{p \in \mathcal{P}({T}_1)} \sum_{q \in \mathcal{P}(\mathbb{T}_2)} d(p,q) \, x_{pq} 
    + \sum_{p \in \mathcal{P}(\mathbb{T}_1)} w(p) \, x_{p} 
\Bigg] \\
\text{s.t.} \quad 
& \sum_{q \in \mathcal{P}(\mathbb{T}_2)} x_{pq} + x_{p} = 1, 
\quad \forall p \in \mathcal{P}(\mathbb{T}_1) \\
& \sum_{p \in \mathcal{P}(\mathbb{T}_1)} x_{pq} = 1, 
\quad \forall q \in \mathcal{P}(\mathbb{T}_2) \\
\\
& x_{pq} \in \{0,1\}, \quad x_{p} \in \{0,1\},
\\
& \quad \forall p \in \mathcal{P}(\mathbb{T}_1),
\quad \forall q \in \mathcal{P}(\mathbb{T}_2)
\end{align*}
$$

## Configuration Parameters

```{python}
#| echo: True
#| code-overflow: wrap

DEPTHS = list(range(3, 13))
MIN_SAMPLES = [3, 5, 10, 30, 50]

NUM_BOOTSTRAPS = 25
DATA_PATH = "../data/DataSet_Combined_SI_SNI_Baseline_FE.csv"

FEATURE_SETS: Dict[str, List[str]] = {
    "suicidea": [
        "age", "gender", "sexori", "raceall", "trauma_sum", "cesd_score", "harddrug_life", "school", "degree", "job", "sex", "concurrent", "exchange", "children", "weapon", "fight", "fighthurt", "ipv", "ptsd_score", "alcfirst", "potfirst", "staycurrent", "homelage", "time_homeless_month", "jail", "jailkid", "gettherapy", "sum_alter", "sum_family", "sum_home_friends", "sum_street_friends", "sum_unknown_alter", "sum_talk_once_week", "sum_alter3close", "prop_family_harddrug", "prop_friends_harddrug", "prop_friends_home_harddrug", "prop_friends_street_harddrug", "prop_alter_all_harddrug", "prop_enc_badbehave", "prop_alter_homeless", "prop_family_emosup", "prop_friends_emosup", "prop_friends_home_emosup", "prop_friends_street_emosup", "prop_alter_all_emosup", "prop_family_othersupport", "prop_friends_othersupport", "prop_friends_home_othersupport", "prop_friends_street_othersupport", "prop_alter_all_othersupport", "sum_alter_staff", "prop_object_badbehave", "prop_enc_goodbehave", "prop_alter_school_job", "sum_alter_borrow"],
    "suicattempt": [
        "age", "gender", "sexori", "raceall", "trauma_sum", "cesd_score", "harddrug_life", "school", "degree", "job", "sex", "concurrent", "exchange", "children", "weapon", "fight", "fighthurt", "ipv", "ptsd_score", "alcfirst", "potfirst", "staycurrent", "homelage", "time_homeless_month", "jail", "jailkid", "gettherapy", "sum_alter", "prop_family", "prop_home_friends", "prop_street_friends", "prop_unknown_alter", "sum_talk_once_week", "sum_alter3close", "prop_family_harddrug", "prop_friends_harddrug", "prop_friends_home_harddrug", "prop_friends_street_harddrug", "prop_alter_all_harddrug", "prop_enc_badbehave", "prop_alter_homeless", "prop_family_emosup", "prop_friends_emosup", "prop_friends_home_emosup", "prop_friends_street_emosup", "prop_alter_all_emosup", "prop_family_othersupport", "prop_friends_othersupport", "prop_friends_home_othersupport", "prop_friends_street_othersupport", "prop_alter_all_othersupport", "sum_alter_staff", "prop_object_badbehave", "prop_enc_goodbehave", "prop_alter_school_job", "sum_alter_borrow"],
}

MODEL_PARAMS = {
    "suicidea": dict(min_samples_leaf=10, min_samples_split=20, max_depth=4),
    "suicattempt": dict(min_samples_leaf=10, min_samples_split=30, max_depth=4),
}

LABELS = ["suicidea", "suicattempt"]
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Data Import and Preparation

```{python}
#| label: import-data
#| echo: true
def prepare_data(df: pd.DataFrame, features: List[str], label: str, rng, imbalance=None):
    df = df.replace('NaN', pd.NA)  # replace the string 'NaN' with actual NaN values
    df_full_cleaned = df[features + [label]].dropna().copy()
    X = df_full_cleaned[features]
    y = df_full_cleaned[label]

    if imbalance == "SMOTE":
        sm = SVMSMOTE(random_state=42)
        X, y = sm.fit_resample(X, y)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rng.integers(0, 2**32 - 1), stratify=y)
    X_full = df_full_cleaned[features]
    y_full = df_full_cleaned[label]
    return X_full, y_full, X_train, X_test, y_train, y_test
```

```{python}
#| echo: false
df = pd.read_csv(DATA_PATH)
X_full, y_full, X_train, X_test, y_train, y_test = prepare_data(df, FEATURE_SETS["suicidea"], "suicidea", np.random.default_rng(1234))

print(f"Number of samples in the full dataset: {len(X_full)}")
print(f"Number of samples in the training set: {len(X_train)}")
print(f"Number of samples in the test set: {len(X_test)}")
print(f"Shape of training set: {X_train.shape}")
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Data Imbalance

```{python}
#| fig-cap: Before and after using SMOTE on imbalanced `suicidea`
#| fig-align: center
#| echo: false
#| output-location: column

orig_counts = y_full.value_counts().sort_index()
sm = SVMSMOTE(random_state=42)
X_smote, y_smote = sm.fit_resample(X_full, y_full)
smote_counts = y_smote.value_counts().sort_index()

fig, (ax1, ax2) = plt.subplots(
    ncols=2,
    figsize=(12, 5),
    dpi=150,
)

ax1.hist(
    y_full,
    bins=len(orig_counts),
    rwidth=0.8,
    color='skyblue',
    edgecolor='black'
)
ax1.set_title("Original label distribution")
ax1.set_xlabel("Class label")
ax1.set_ylabel("Count")

ax2.hist(
    y_smote,
    bins=len(smote_counts),
    rwidth=0.8,
    color='salmon',
    edgecolor='black'
)
ax2.set_title("After SMOTE")
ax2.set_xlabel("Class label")
ax2.set_ylabel("Count")
plt.tight_layout()
plt.show()
```

## A peek at the data {.smaller}

```{python}     
#| echo: false
X_train
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Random Train Split

```{python}
#| label: train-test-split
#| echo: true
def random_train_split(X, y):
    X_values = X.values if hasattr(X, 'values') else X
    y_values = y.values if hasattr(y, 'values') else y
    
    N = X_values.shape[0]
    indices = np.random.permutation(N)
    X0, y0 = X_values[indices[:N // 2]], y_values[indices[:N // 2]]
    return X0, y0

X0, y0 = random_train_split(X_train.values, y_train.values)
X0.shape, y0.shape
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Bootstrapping Decision Trees


```{python}
#| echo: true
#| code-line-numbers: "|6,7|10|11,12"
def train_decision_tree(X, y, depth, min_samples_leaf):
    clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf)
    clf.fit(X, y)
    return clf

def bootstrap_trees(X, y, depths, min_samples, B):
    # Create B bootstrap trees by sampling with replacement from X_0
    trees = []
    for _ in range(B):
        X_sample, y_sample = resample(X, y, replace=True)
        depth = np.random.choice(depths)
        min_leaf = np.random.choice(min_samples)
        tree = train_decision_tree(X_sample, y_sample, depth, min_leaf)
        trees.append(tree)
    return trees
```

```{python}
#| echo: false
T0 = bootstrap_trees(X0, y0, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS)
print("Number of trees in T0:", len(T0))

T = bootstrap_trees(X_train.values, y_train.values, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS)
print("Number of trees in T:", len(T))
```

## Computing Tree Distances

```{python}
#| eval: false
#| echo: true
#| code-line-numbers: "6,8,9,10,12"

def compute_average_distances(T0, T, X_train, y_train):
    X_train_values = X_train.values if hasattr(X_train, "values") else X_train
    distances: list[float] = []

    with alive_bar(len(T), title="Computing average tree distances", dual_line=True, spinner="waves", bar="smooth") as bar:
        for tree_b in T:
            d_b = 0.0
            for tree_beta in T0:
                calc = DistanceCalculator(tree_beta, tree_b, X=X_train_values, y=y_train)
                d_b += calc.compute_tree_distance()

            mean_dist = d_b / len(T0)
            distances.append(mean_dist)
            bar()

    return distances
distances = compute_average_distances(T0, T, X_train, y_train)
```

## Evaluating Predictive Power

```{python}
#| eval: true
# load the distances from a csv
distances = []
with open("distances.csv", "r") as f:
    for line in f:
        distances.append(float(line.strip()))
```

```{python}
#| eval: false
# save the distances into a csv
with open("distances.csv", "w") as f:
    for distance in distances:
        f.write(f"{distance}\n")
```

```{python}
#| echo: true
def evaluate_predictive_power(trees, X_holdout, y_holdout):
    auc_scores = []
    for tree in trees:
        y_prob = tree.predict_proba(X_holdout)[:, 1]
        auc = roc_auc_score(y_holdout, y_prob)
        auc_scores.append(auc)
    return auc_scores

auc_scores = evaluate_predictive_power(T, X_test.values, y_test.values)
print("Average AUC score:", np.mean(auc_scores))
```

## Finding Pareto Optimal Trees

- **Pareto Optimal:** $(d_{b'} \leq d_b \text{ and } \alpha_{b'} > \alpha_b) \text{ or } (d_{b'} < d_b \text{ and } \alpha_{b'} \geq \alpha_b)$

```{python}
#| echo: true
#| code-line-numbers: "5,6"
def pareto_optimal_trees(distances, auc_scores):
    pareto_trees = []
    for i, (d_i, a_i) in enumerate(zip(distances, auc_scores)):
        dominated = False
        for j, (d_j, a_j) in enumerate(zip(distances, auc_scores)):
            if i != j and ((d_j <= d_i and a_j > a_i) or (d_j < d_i and a_j >= a_i)):
                dominated = True
                break
        if not dominated:
            pareto_trees.append(i)
    return pareto_trees

pareto_trees = pareto_optimal_trees(distances, auc_scores)
print("Number of Pareto optimal trees:", len(pareto_trees))
```

## Tree Selection Strategy

-  $\mathbb{T}^\star = \underset{\mathbb{T}_b \in \mathcal{T}^\star}{\text{argmax}} \ f(d_b, \alpha_b)$
- Stability or predictive power?
- Indicator function: $\alpha_{b}$ is within Œµ of the best score

```{python}
#| echo: true
#| code-line-numbers: "3"
def select_final_tree(distances, auc_scores, pareto_indices, epsilon=0.01):
    best_auc = max(auc_scores)
    candidates = [i for i in pareto_indices if auc_scores[i] >= (1 - epsilon) * best_auc]
    if not candidates:
        candidates = pareto_indices
    best_idx = max(candidates, key=lambda i: auc_scores[i] - distances[i])
    return best_idx

selected_tree_index = select_final_tree(distances, auc_scores, pareto_trees)
print("Selected tree index:", selected_tree_index)
```

## Visualizing Selected Tree

```{python}
#| fig-align: center

selected_tree = T[selected_tree_index]
plt.figure(figsize=(12, 8), dpi=500)
plot_tree(selected_tree, 
          feature_names=X_full.columns,
          class_names=["No Suicide Ideation", "Suicide Ideation"],
          filled=True)
plt.tight_layout()
```

## Pareto Frontier Visualization

:::: {.columns}

::: {.column width="55%"}
```{python}
def plot_pareto_frontier(distances, auc_scores, pareto_indices):
    distances = np.array(distances)
    auc_scores = np.array(auc_scores)
    pareto_indices = set(pareto_indices)
    is_pareto = np.array([i in pareto_indices for i in range(len(distances))])
    # Plotting
    plt.figure(figsize=(8, 7), dpi=500)
    plt.scatter(distances[~is_pareto], auc_scores[~is_pareto], c='blue', label='Dominated Trees', alpha=0.6)
    plt.scatter(distances[is_pareto], auc_scores[is_pareto], c='red', edgecolors='black', s=80, label='Pareto Optimal Trees')
    plt.xlabel("Stability (Lower is Better)")
    plt.ylabel("AUC (Higher is Better)")
    plt.title("Pareto Frontier of Decision Trees")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

plot_pareto_frontier(distances, auc_scores, pareto_trees)
```
:::

::: {.column width="45%"}

**Average distance**

$d_{b}$ , $\forall b \in \mathcal{T}$

\
**Out-of-sample AUCROC**

$a_{b}$, $\forall b \in \mathcal{T}$
:::

::::

## Aggregated metrics {.smaller}


|         Method        	|      AUC      	|    Distance   	| Feat. Import. Std. 	| Feat. in Top-3 	| Nodes 	| Tree Depth 	|
|:---------------------:	|:-------------:	|:-------------:	|:------------------:	|:--------------:	|:-----:	|:----------:	|
| CART Pareto AUC       	| 0.752 (0.029) 	| 0.043 (0.009) 	|        0.028       	|       11       	|  14.8 	|     4.4    	|
| CART Pareto Stability 	| 0.751 (0.029) 	| 0.037 (0.014) 	|        0.03        	|       12       	|  19.2 	|     5.0    	|
| CART Pareto Distance  	| 0.596 (0.066) 	| 0.009 (0.005) 	|        0.026       	|       20       	|  49.0 	|     8.4    	|



## Experiment runner {.smaller}

::: {.panel-tabset}

### seed=23
```{bash}
‚ùØ uv run src/StableTree/main.py --seeds 23 28 --group-name seed_checking
Created experiment group: seed_checking
==================================================
Running experiment with seed 23
==================================================
Experiment: experiment_20250429_183236_seed_23 - Seed: 23
Number of samples in the full dataset: 586
Number of samples in the training set: 439
Number of samples in the test set: 147
Shape of training set: (439, 56)
Shape of random split: (219, 56), (219,)
Number of trees in T0: 20
Number of trees in T: 20
Computing average tree distances |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [100\%] in 10.8s (1.76/s) 
Number of distances computed: 20
Average AUC score: 0.5955578512396694
Number of Pareto optimal trees: 7
Frequenicies of top 2 common features: [[(18, 30.0), (5, 20.0)], [(6, 40.0), (5, 20.0)], [(6, 20.0), (14, 10.0)]]
Selected tree index: 16
Completed experiment: experiment_20250429_183236_seed_23
```

### seed=28
```{bash}
==================================================
Running experiment with seed 28
==================================================
Experiment: experiment_20250429_183249_seed_28 - Seed: 28
Number of samples in the full dataset: 586
Number of samples in the training set: 439
Number of samples in the test set: 147
Shape of training set: (439, 56)
Shape of random split: (219, 56), (219,)
Number of trees in T0: 20
Number of trees in T: 20
Computing average tree distances |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [100\%] in 10.6s (1.89/s)
Number of distances computed: 20
Average AUC score: 0.6365702479338843
Number of Pareto optimal trees: 5
Frequenicies of top 2 common features: [[(5, 30.0), (6, 30.0)], [(6, 25.0), (5, 20.0)], [(4, 15.0), (54, 15.0)]]
Selected tree index: 3
Completed experiment: experiment_20250429_183249_seed_28
Experiment group summary saved to experiments/seed_checking/group_summary.json
Total experiments run: 2
```
:::

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Thoughts and points of dicussion {.smaller}

1. Does the sequence of paths matter? @10571918

> The study of [39] proposes a new distance metric to quantify the structural differences and prediction similarities between decision trees. ... However, the metric does not consider the sequence of splits, thereby potentially overlooking the overall structural similarity of the trees. Furthermore, the approach suffers from high computational complexity due to the need for pairwise comparison of all paths between two trees, which can become computationally expensive as the number of leaf nodes increases.

2. Stability-performance trade-off when selecting Pareto Optimal tree?

## References

