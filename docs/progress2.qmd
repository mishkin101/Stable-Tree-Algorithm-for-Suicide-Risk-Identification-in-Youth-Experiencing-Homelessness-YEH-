---
title: "Decision Tree Stability for Suicide Experience Prediction"
subtitle: "ISE 625 Project Presentation"
format:
    revealjs:
        theme: [default, template/custom.scss]
        slide-number: c/t
#   clean-revealjs:
#     theme: [default, template/code.scss]
    # self-contained: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: "**Adhithya Bhaskar**"
  - name: "**Michelle Gelman**"
execute: 
  cache: false
jupyter: python3
df-print: paged
date: last-modified
code-line-numbers: false
highlight-style: a11y
bibliography: refs.bib
csl: template/ieee.csl
---

```{python}
import sys
import itertools
from pathlib import Path
from typing import Dict, List
from itables import show
from IPython.display import Markdown
src_path = Path("../src/dt-distance").resolve()
data_path = Path("../data").resolve()
sys.path.append(str(data_path))
sys.path.append(str(src_path))

import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import roc_auc_score
from sklearn.utils import resample
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from dt_distance.distance_calculator import DistanceCalculator
import matplotlib.pyplot as plt
from alive_progress import alive_bar
from imblearn.over_sampling import SMOTE, SVMSMOTE
np.random.seed(42)
DATA_PATH = "../data/DataSet_Combined_SI_SNI_Baseline_FE.csv"
```

```{python}
#| echo: false
DEPTHS = list(range(3, 13))
MIN_SAMPLES = [3, 5, 10, 30, 50]
NUM_BOOTSTRAPS = 25

FEATURE_SETS: Dict[str, List[str]] = {
    "suicidea": [
        "age", "gender", "sexori", "raceall", "trauma_sum", "cesd_score", "harddrug_life", "school", "degree", "job", "sex", "concurrent", "exchange", "children", "weapon", "fight", "fighthurt", "ipv", "ptsd_score", "alcfirst", "potfirst", "staycurrent", "homelage", "time_homeless_month", "jail", "jailkid", "gettherapy", "sum_alter", "sum_family", "sum_home_friends", "sum_street_friends", "sum_unknown_alter", "sum_talk_once_week", "sum_alter3close", "prop_family_harddrug", "prop_friends_harddrug", "prop_friends_home_harddrug", "prop_friends_street_harddrug", "prop_alter_all_harddrug", "prop_enc_badbehave", "prop_alter_homeless", "prop_family_emosup", "prop_friends_emosup", "prop_friends_home_emosup", "prop_friends_street_emosup", "prop_alter_all_emosup", "prop_family_othersupport", "prop_friends_othersupport", "prop_friends_home_othersupport", "prop_friends_street_othersupport", "prop_alter_all_othersupport", "sum_alter_staff", "prop_object_badbehave", "prop_enc_goodbehave", "prop_alter_school_job", "sum_alter_borrow"],
    "suicattempt": [
        "age", "gender", "sexori", "raceall", "trauma_sum", "cesd_score", "harddrug_life", "school", "degree", "job", "sex", "concurrent", "exchange", "children", "weapon", "fight", "fighthurt", "ipv", "ptsd_score", "alcfirst", "potfirst", "staycurrent", "homelage", "time_homeless_month", "jail", "jailkid", "gettherapy", "sum_alter", "prop_family", "prop_home_friends", "prop_street_friends", "prop_unknown_alter", "sum_talk_once_week", "sum_alter3close", "prop_family_harddrug", "prop_friends_harddrug", "prop_friends_home_harddrug", "prop_friends_street_harddrug", "prop_alter_all_harddrug", "prop_enc_badbehave", "prop_alter_homeless", "prop_family_emosup", "prop_friends_emosup", "prop_friends_home_emosup", "prop_friends_street_emosup", "prop_alter_all_emosup", "prop_family_othersupport", "prop_friends_othersupport", "prop_friends_home_othersupport", "prop_friends_street_othersupport", "prop_alter_all_othersupport", "sum_alter_staff", "prop_object_badbehave", "prop_enc_goodbehave", "prop_alter_school_job", "sum_alter_borrow"],
}

MODEL_PARAMS = {
    "suicidea": dict(min_samples_leaf=10, min_samples_split=20, max_depth=4),
    "suicattempt": dict(min_samples_leaf=10, min_samples_split=30, max_depth=4),
}

LABELS = ["suicidea", "suicattempt"]
```

## Problem outline

- 4.2 million youth experience homelessness each year in the U.S
- **~27%** and **~35%** of **R**unaway and **H**omeless **Y**outh report past-year suicidal ideation @kirstConcurrentMentalHealth2011 @rewSexualAbuseAlcohol2001 compared with **15.8%** in general population @YouthRiskBehaviora
- **>50%** of RHY have experienced suicidal ideation during their lifetime @merschamMentalHealthSubstance2009 @molnarSuicidalBehaviorSexual1998 @vottaSuicideHighriskBehaviors2004

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Key question

- Most studies focus on **individual characteristics** in suicidal risk for YEH @PreventingSuicideConnectedness
- Limited insight on relevant factors and combinations of both individual and social factors that influence suicide risk @fulginitiRiskyIntegrationSocial2016
<!-- - **Novel** No previous social network considerations or behavior indicators in homeless population suicide research -->
- **Risk Profile:** Signal of high vulnerability or resiliency with respect to suicidal ideation and attempts 

**Can a stable tree be used to understand the clinical risk profiles of YEH?**

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Data Collection & considerations

- **Survey:** 
    - Use individual and social network attributes
    - Between October 2011 and February 2013
    - 2 drop-in centers that serve YEH in Hollywood and Santa Monica, CA
- **Current Factors:** 
    - Overrepresentation of males and heterosexual youth
    - Generalizability to other geographical regions

::: footer
<a style="opacity: 0.2;">M</a>
:::

## What does **trust** mean to a clinician?

- **Accurate intervention targets $\rightarrow$ ** Help stakeholders understand how to build intervention strategies based on clinical risk profiles for YEH

- **Less algorithmic aversion $\rightarrow$ ** Clinicians' decision making is not affected by unreliable models' decisions from a lack of consistency in identifying suicidal risk

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Why do we need **stable trees**?

- Population characteristics *change over time*
- Model‚Äôs predicted output should not change with subsequent runs in light of new patient data
- Initial dataset is small, but larger amounts of data become available over time as more patients‚Äô information gets recorded **$\rightarrow$** retraining needed @bertsimasImprovingStabilityDecision2023

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Data Import and Preparation

```{python}
#| label: import-data
#| echo: true
def prepare_data(df: pd.DataFrame, features: List[str], label: str, rng, imbalance=None):
    df = df.replace('NaN', pd.NA)  # replace the string 'NaN' with actual NaN values
    df_full_cleaned = df[features + [label]].dropna().copy()
    X = df_full_cleaned[features]
    y = df_full_cleaned[label]

    if imbalance == "SMOTE":
        sm = SVMSMOTE(random_state=42)
        X, y = sm.fit_resample(X, y)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rng.integers(0, 2**32 - 1), stratify=y)
    X_full = df_full_cleaned[features]
    y_full = df_full_cleaned[label]
    return X_full, y_full, X_train, X_test, y_train, y_test
```

```{python}
#| echo: false
df = pd.read_csv(DATA_PATH)
X_full, y_full, X_train, X_test, y_train, y_test = prepare_data(df, FEATURE_SETS["suicidea"], "suicidea", np.random.default_rng(1234))

print(f"Number of samples in the full dataset: {len(X_full)}")
print(f"Number of samples in the training set: {len(X_train)}")
print(f"Number of samples in the test set: {len(X_test)}")
print(f"Shape of training set: {X_train.shape}")
```

## Data Imbalance

```{python}
#| fig-cap: Before and after using SMOTE on imbalanced `suicidea`
#| fig-align: center
#| echo: false
orig_counts = y_full.value_counts().sort_index()
sm = SVMSMOTE(random_state=42)
X_smote, y_smote = sm.fit_resample(X_full, y_full)
smote_counts = y_smote.value_counts().sort_index()

fig, (ax1, ax2) = plt.subplots(
    ncols=2,
    figsize=(12, 5),
    dpi=150,
)

ax1.hist(
    y_full,
    bins=len(orig_counts),
    rwidth=0.8,
    color='skyblue',
    edgecolor='black'
)
ax1.set_title("Original label distribution")
ax1.set_xlabel("Class label")
ax1.set_ylabel("Count")

ax2.hist(
    y_smote,
    bins=len(smote_counts),
    rwidth=0.8,
    color='salmon',
    edgecolor='black'
)
ax2.set_title("After SMOTE")
ax2.set_xlabel("Class label")
ax2.set_ylabel("Count")
plt.tight_layout()
plt.show()
```

## A peek at the data {.smaller}

```{python}     
#| echo: false
X_train
```

## Tree Path

*Sequence of splits from the root to a leaf & class label $k^p$*

$$
ùí´(ùïã) = \{p_1, \dots, p_T\}
$$

- **$\mathbf{u}^p_j$**, **$\mathbf{l}^p_j$**: define the numeric interval for feature $j$  
- **$\mathbf{c}^p_j$**: binary vector indicating which categories of feature $j$ satisfy the splits  
- **$\mathbf{k}^p$**: class label predicted for this region

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Tree Distance {.smaller}
 $$
\begin{align*}
d({T}_1, {T}_2)
&= \min_{{x}} \Bigg[ 
    \sum_{p \in \mathcal{P}({T}_1)} \sum_{q \in \mathcal{P}(\mathbb{T}_2)} d(p,q) \, x_{pq} 
    + \sum_{p \in \mathcal{P}(\mathbb{T}_1)} w(p) \, x_{p} 
\Bigg] \\
\text{s.t.} \quad 
& \sum_{q \in \mathcal{P}(\mathbb{T}_2)} x_{pq} + x_{p} = 1, 
\quad \forall p \in \mathcal{P}(\mathbb{T}_1) \\
& \sum_{p \in \mathcal{P}(\mathbb{T}_1)} x_{pq} = 1, 
\quad \forall q \in \mathcal{P}(\mathbb{T}_2) \\
\\
& x_{pq} \in \{0,1\}, \quad x_{p} \in \{0,1\},
\\
& \quad \forall p \in \mathcal{P}(\mathbb{T}_1),
\quad \forall q \in \mathcal{P}(\mathbb{T}_2)
\end{align*}
$$

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Stable decision trees {.smaller}

Implementing stable trees proposed by Bertsimas et al. 2023 @bertsimasImprovingStabilityDecision2023

1. **Initial Training (T0):** Train initial set of decision trees on subset
2. **Full Data Training (T):** Train a second set on full training data 
3. **Distance Computation:**  Calulate average distance between trees in **T** and the trees in **T0**
  $$
  d\bigl(\mathcal{T}_{1}, \mathcal{T}_{2}\bigr) \;=\;\min_{\{x\}}\ \sum_{p\in\mathcal{P}(\mathcal{T}_{1})}\sum_{q\in\mathcal{P}(\mathcal{T}_{2})} d(p,q)\, x_{p,q} \;+\;\sum_{p\in\mathcal{P}(\mathcal{T}_{1})} w(p)\, x_{p}
  $$
1. **Performance Metrics:** Compute AUC ROC on test set
2. **Pareto Optimization:** Select Pareto optimal trees that balance predictive performance and stability
  $$
  \mathbb{T}^{\star}=\arg\!\mathrm{max}\,f\!\left(d_{b},\alpha_{b}\right)\!
  $$

::: footer
<a style="opacity: 0.2;">A</a>
:::

## 

\
\

![](./amethod%20diagram.svg){fig-align="center"}

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Random Train Split

```{python}
#| label: train-test-split
#| echo: true
def random_train_split(X, y):
    X_values = X.values if hasattr(X, 'values') else X
    y_values = y.values if hasattr(y, 'values') else y
    
    N = X_values.shape[0]
    indices = np.random.permutation(N)
    X0, y0 = X_values[indices[:N // 2]], y_values[indices[:N // 2]]
    return X0, y0

X0, y0 = random_train_split(X_train.values, y_train.values)
X0.shape, y0.shape
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Bootstrapping Decision Trees


```{python}
#| echo: true
#| code-line-numbers: "6,7,10,11,12"
def train_decision_tree(X, y, depth, min_samples_leaf):
    clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=min_samples_leaf)
    clf.fit(X, y)
    return clf

def bootstrap_trees(X, y, depths, min_samples, B):
    # Create B bootstrap trees by sampling with replacement from X_0
    trees = []
    for _ in range(B):
        X_sample, y_sample = resample(X, y, replace=True)
        depth = np.random.choice(depths)
        min_leaf = np.random.choice(min_samples)
        tree = train_decision_tree(X_sample, y_sample, depth, min_leaf)
        trees.append(tree)
    return trees
```

::: footer
<a style="opacity: 0.2;">A</a>
:::


```{python}
#| echo: false
T0 = bootstrap_trees(X0, y0, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS)
print("Number of trees in T0:", len(T0))

T = bootstrap_trees(X_train.values, y_train.values, DEPTHS, MIN_SAMPLES, NUM_BOOTSTRAPS)
print("Number of trees in T:", len(T))
```

## Computing Tree Distances

```{python}
#| eval: false
#| echo: true
#| code-line-numbers: "6,8,9,10,12"

def compute_average_distances(T0, T, X_train, y_train):
    X_train_values = X_train.values if hasattr(X_train, "values") else X_train
    distances: list[float] = []

    with alive_bar(len(T), title="Computing average tree distances", dual_line=True, spinner="waves", bar="smooth") as bar:
        for tree_b in T:
            d_b = 0.0
            for tree_beta in T0:
                calc = DistanceCalculator(tree_beta, tree_b, X=X_train_values, y=y_train)
                d_b += calc.compute_tree_distance()

            mean_dist = d_b / len(T0)
            distances.append(mean_dist)
            bar()

    return distances
distances = compute_average_distances(T0, T, X_train, y_train)
```

::: footer
<a style="opacity: 0.2;">A</a>
:::


<!-- ## Evaluating Predictive Power -->

```{python}
#| eval: true
# load the distances from a csv
distances = []
with open("distances.csv", "r") as f:
    for line in f:
        distances.append(float(line.strip()))
```

```{python}
#| eval: false
# save the distances into a csv
with open("distances.csv", "w") as f:
    for distance in distances:
        f.write(f"{distance}\n")
```

```{python}
#| echo: false
def evaluate_predictive_power(trees, X_holdout, y_holdout):
    auc_scores = []
    for tree in trees:
        y_prob = tree.predict_proba(X_holdout)[:, 1]
        auc = roc_auc_score(y_holdout, y_prob)
        auc_scores.append(auc)
    return auc_scores

auc_scores = evaluate_predictive_power(T, X_test.values, y_test.values)
# print("Average AUC score:", np.mean(auc_scores))
```

## Finding Pareto Optimal Trees

$(d_{b'} \leq d_b \text{ and } \alpha_{b'} > \alpha_b)$
*or* 
$(d_{b'} < d_b \text{ and } \alpha_{b'} \geq \alpha_b)$

```{python}
#| echo: true
#| code-line-numbers: "5,6"
def pareto_optimal_trees(distances, auc_scores):
    pareto_trees = []
    for i, (d_i, a_i) in enumerate(zip(distances, auc_scores)):
        dominated = False
        for j, (d_j, a_j) in enumerate(zip(distances, auc_scores)):
            if i != j and ((d_j <= d_i and a_j > a_i) or (d_j < d_i and a_j >= a_i)):
                dominated = True
                break
        if not dominated:
            pareto_trees.append(i)
    return pareto_trees

pareto_trees = pareto_optimal_trees(distances, auc_scores)
print("Number of Pareto optimal trees:", len(pareto_trees))
```

::: footer
<a style="opacity: 0.2;">A</a>
:::


## Tree Selection Strategy

-  $\mathbb{T}^\star = \underset{\mathbb{T}_b \in \mathcal{T}^\star}{\text{argmax}} \ f(d_b, \alpha_b)$
- **Tradeoff:** Stability *and* predictive power
<!-- - Indicator function: $\alpha_{b}$ is within Œµ of the best score -->

```{python}
#| echo: true
#| code-line-numbers: "3"
def select_final_tree(distances, auc_scores, pareto_indices, epsilon=0.01):
    best_auc = max(auc_scores)
    candidates = [i for i in pareto_indices if auc_scores[i] >= (1 - epsilon) * best_auc]
    if not candidates:
        candidates = pareto_indices
    best_idx = max(candidates, key=lambda i: auc_scores[i] - distances[i])
    return best_idx

selected_tree_index = select_final_tree(distances, auc_scores, pareto_trees)
print("Selected tree index:", selected_tree_index)
```

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Experiment runner {.smaller}

::: {.panel-tabset}

### `label=suicidea`
```{bash}
==================================================
Running for dataset DataSet_Combined_SI_SNI_Baseline_FE with seed 42
==================================================
ds_nameDataSet_Combined_SI_SNI_Baseline_FE.csv
Experiment: experiment_20250501_134848_seed_42_DataSet_Combined_SI_SNI_Baseline_FE_suicidea - Seed: 42 - Dataset: DataSet_Combined_SI_SNI_Baseline_FE
Number of samples in the full dataset: 586
Number of samples in the training set: 726
Number of samples in the test set: 242
Shape of training set: (726, 56)
Shape of random split: (363, 56), (363,)
Number of trees in T0: 20
Number of trees in T: 20
Computing average tree distances |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [100%] in 20.7s (0.96/s) 
Number of distances computed: 20
Average AUC score: 0.821854723038044
Number of Pareto optimal trees: 7
Frequenicies of top 2 common features: [[('trauma_sum', 70.0), ('fight', 20.0)], [('harddrug_life', 45.0), ('exchange', 15.0)], [('LEAF_NODE', 25.0), ('harddrug_life', 20.0)]]
Selected stability-accuracy trade-off final tree index: 1
Stability-accuracy tree depth: 4, nodes: 23
Selected AUC maximizing tree index: 1
AUC-maximizing tree depth: 4, nodes: 23
Selected distance minimizing tree index: 15
Distance-minimizing tree depth: 11, nodes: 79
Completed experiment: 
```

### `label=suicattempt`
```{bash}
==================================================
Running for dataset DataSet_Combined_SI_SNI_Baseline_FE with seed 42
==================================================
ds_nameDataSet_Combined_SI_SNI_Baseline_FE.csv
Experiment: experiment_20250501_134911_seed_42_DataSet_Combined_SI_SNI_Baseline_FE_suicattempt - Seed: 42 - Dataset: DataSet_Combined_SI_SNI_Baseline_FE
Number of samples in the full dataset: 587
Number of samples in the training set: 627
Number of samples in the test set: 209
Shape of training set: (627, 56)
Shape of random split: (313, 56), (313,)
Number of trees in T0: 20
Number of trees in T: 20
Computing average tree distances |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [100%] in 9.4s (2.13/s) 
Number of distances computed: 20
Average AUC score: 0.8648953261927945
Number of Pareto optimal trees: 5
Frequenicies of top 2 common features: [[('fighthurt', 45.0), ('fight', 35.0)], [('trauma_sum', 40.0), ('degree', 15.0)], [('gettherapy', 15.0), ('exchange', 15.0)]]
Selected stability-accuracy trade-off final tree index: 18
Stability-accuracy tree depth: 7, nodes: 59
Selected AUC maximizing tree index: 8
AUC-maximizing tree depth: 4, nodes: 19
Selected distance minimizing tree index: 11
Distance-minimizing tree depth: 10, nodes: 61
Completed experiment: experiment_20250501_134911_seed_42_DataSet_Combined_SI_SNI_Baseline_FE_suicattempt
```

### `label=target`
```{bash}
==================================================
Running for dataset breast_cancer with seed 42
==================================================
ds_namebreast_cancer.csv
Experiment: experiment_20250501_134921_seed_42_breast_cancer_target - Seed: 42 - Dataset: breast_cancer
Number of samples in the full dataset: 569
Number of samples in the training set: 535
Number of samples in the test set: 179
Shape of training set: (535, 30)
Shape of random split: (267, 30), (267,)
Number of trees in T0: 20
Number of trees in T: 20
Computing average tree distances |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [100%] in 1.9s (10.51/s) 
Number of distances computed: 20
Average AUC score: 0.9502996254681648
Number of Pareto optimal trees: 5
Frequenicies of top 2 common features: [[('worst perimeter', 65.0), ('mean concave points', 20.0)], [('worst smoothness', 15.0), ('worst radius', 15.0)], [('area error', 30.0), ('worst texture', 10.0)]]
Selected stability-accuracy trade-off final tree index: 14
Stability-accuracy tree depth: 5, nodes: 19
Selected AUC maximizing tree index: 12
AUC-maximizing tree depth: 3, nodes: 9
Selected distance minimizing tree index: 13
Distance-minimizing tree depth: 6, nodes: 23
Completed experiment: experiment_20250501_134921_seed_42_breast_cancer_target
```
:::

::: footer
<a style="opacity: 0.2;">A</a>
:::

## Visualizing Selected Tree

```{python}
#| fig-align: center
selected_tree = T[selected_tree_index]
plt.figure(figsize=(12, 8), dpi=500)
plot_tree(selected_tree, 
          feature_names=X_full.columns,
          class_names=["No Suicide Ideation", "Suicide Ideation"],
          filled=True)
plt.tight_layout()
```

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Aggregated metrics

\
\

![](./agg_table.png){fig-align="center"}

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Results - Aggregated STD

![](../experiments/FINAL_SMOTICHKIN/aggregate_metrics_feature_std.png){fig-align="center"}

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Results - Tree depth

![](../experiments/FINAL_SMOTICHKIN/aggregate_metrics_tree_depth.png){fig-align="center"}

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Results - Number of nodes

![](../experiments/FINAL_SMOTICHKIN/aggregate_metrics_tree_nodes.png){fig-align="center"}

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Results - Distinct top features

![](../experiments/FINAL_SMOTICHKIN/aggregate_metrics_distinct_top_features.png){fig-align="center"}

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Results - Current Considerations

- Correlation to other aggregate metrics with magnitude of distance?
- What the "right depth?"" (bias-variance trade off)
    - optimizing for distance -> deeper trees
- Is T0 large enough currently statistically to represent T when bootstrapping?
    - Checks for distribution shift in sampling

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Thinking About Stability

\

:::: {.columns}

::: {.column width="50%"}

![](../logs/experiment_20250501_134921_seed_42_breast_cancer_target/plots/decision_tree.png){fig-align="center"}

:::

::: {.column width="50%"}

![](../logs/experiment_20250501_134911_seed_42_DataSet_Combined_SI_SNI_Baseline_FE_suicattempt/plots/decision_tree.png){fig-align="center"}

:::

::::

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Pareto Frontier Visualization

:::: {.columns}

::: {.column width="55%"}
```{python}
def plot_pareto_frontier(distances, auc_scores, pareto_indices):
    distances = np.array(distances)
    auc_scores = np.array(auc_scores)
    pareto_indices = set(pareto_indices)
    is_pareto = np.array([i in pareto_indices for i in range(len(distances))])
    # Plotting
    plt.figure(figsize=(8, 7), dpi=500)
    plt.scatter(distances[~is_pareto], auc_scores[~is_pareto], c='blue', label='Dominated Trees', alpha=0.6)
    plt.scatter(distances[is_pareto], auc_scores[is_pareto], c='red', edgecolors='black', s=80, label='Pareto Optimal Trees')
    plt.xlabel("Stability (Lower is Better)")
    plt.ylabel("AUC (Higher is Better)")
    plt.title("Pareto Frontier of Decision Trees")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

plot_pareto_frontier(distances, auc_scores, pareto_trees)
```
:::

::: {.column width="45%"}

**Average distance**

$d_{b}$ , $\forall b \in \mathcal{T}$

\
**Out-of-sample AUCROC**

$a_{b}$, $\forall b \in \mathcal{T}$
:::

::::

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Future work - Short term

- Refine what **stability** means with our stakeholders
    - What do we measure concerning and how do we quantify success?
- Hyper-parameter finetuning
    - Path distance class weight
    - Pareto tree selection strategy
    - Data pre-processing
- Stability-performance trade-off when selecting Pareto Optimal tree?

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Future work - Long term

- Does implementation meet the criteria for reliability in identifying YEH?
- Feature selection and qualitative pre-processing
- Robustness: direct versus indirect perturbation sensitivity analysis
    - **Direct:** Changes in tree structure (threshold pertubations)
    - **Indirect:** Modifications in  training data

::: footer
<a style="opacity: 0.2;">M</a>
:::

## Thoughts and points of dicussion {.smaller}

\

**Does the sequence of paths matter?** @10571918

> The study of [39] proposes a new distance metric to quantify the structural differences and prediction similarities between decision trees. ... However, the metric does not consider the sequence of splits, thereby potentially overlooking the overall structural similarity of the trees. Furthermore, the approach suffers from high computational complexity due to the need for pairwise comparison of all paths between two trees, which can become computationally expensive as the number of leaf nodes increases.

::: footer
<a style="opacity: 0.2;">M</a>
:::

## References

