---
title: "ISE 625 Project Progress Report"
subtitle: "Stable decision trees for suicide experience prediction"
format: 
    revealjs:
        slide-number: c/t
        theme: [default, template/custom.scss]
        toc: true
        progress: true
        # mouse-wheel: true
        controls: true
    pdf:
        toc: true
        colorlinks: true
author: "Adhithya Bhaskar, Michelle Gelman"
# lightbox: true
# embed-resources: true
# bibliography: template/references.bib
# incremental: true
# csl: template/ieee.csl
# width: 
# height: 816
editor:
    render-on-save: true
---

## Problem Context and Background
We aim to develop a model to predict suicidal experiences among youth experiencing homelessness (YEH). The provided decision tree (DT) model for the current YEH dataset shows instability with respect to changes in train-test splits. We aim to address the following:

**Core Question:** Can we design a robust, stable decision tree that remains invariant under shifts in data distributions while still identifying the key features indicative of suicidal ideation and attempts?

## Dataset Considerations

**Missing data**

- (584, 587) samples remaining for each prediction model from initial listwise deletion method form original 940 total samples
- 4% of data set missing for `suicideidea` and `suicideattempt` (36 and 40 samples respectively)

**Imbalanced classes**

- 83% labeled 2, 16% labeled 1 for `suicideidea` class
- 88% labeled 0, 11% labeled 1 for `suicideattempt` class 

## Steps to implementing a Stable Decision Tree (Bertsimas et al. 2023)

  1. **Initial Training (T0):**  
    Train an initial set of decision trees on a subset of the data.
  2. **Full Data Training (T):**  
    Train a second set of decision trees on the full dataset.
  3. **Distance Computation:**  
    Calculate the average distance between each tree in **T** and the trees in **T0**. The distance between two trees is defined as:
    $$
    d\bigl(\mathcal{T}_{1}, \mathcal{T}_{2}\bigr) \;=\;\min_{\{x\}}\ \sum_{p\in\mathcal{P}(\mathcal{T}_{1})}\sum_{q\in\mathcal{P}(\mathcal{T}_{2})} d(p,q)\, x_{p,q} \;+\;\sum_{p\in\mathcal{P}(\mathcal{T}_{1})} w(p)\, x_{p}
    $$

  4. **Performance Metrics:**  
    Compute performance metrics (such as AUC) on a validation/test set.
  5. **Pareto Optimization:**  
    Select Pareto optimal trees that balance predictive performance and stability. This is expressed as:
    $$
    \mathbb{T}^{\star}=\arg\!\mathrm{max}\,f\!\left(d_{b},\alpha_{b}\right)\!
    $$

## Implementation progress:

{{< pagebreak >}}

## Project Outcomes
- A robust, stable decision tree model that minimizes the variability in tree structure due to random train-test splits
- Empirical evidence supporting the stability of the model through consistent feature selection and comparable performance metrics

- **Impact:** Better interpretability of decision trees to predict suicide risk among YEHs